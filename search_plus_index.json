{"./":{"url":"./","title":"Introduction","keywords":"","body":" &#x1F37C; 机器学习记录 其中有些来自一些博客论坛，如有侵权，请联系作者 1832044043@qq.com 其中能加上原文地址的都已在文中尽可能加上 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/机器学习基础.html":{"url":"chapters/机器学习基础.html","title":"机器学习基础","keywords":"","body":"机器学习基础 [TOC] 机器学习基础 机器学习必备的数学基础有哪些？ 机器学习几种方式 大规模机器学习框架的四重境界 参数服务器——分布式机器学习的新杀器 腾讯首个 AI 开源项目 Angel 发布 3.0 版本：迈向全栈机器学习平台 Factorization Machines(FM) 因子分解机和Field-aware Factorization Machine (FFM) 场感知分解机 FM算法解析 因子分解机（FM）与场感知分解机（FFM） https://keysaim.github.io/post/blog/2017-08-15-how-to-setup-your-github-io-blog/ 腾讯发布开源机器学习平台 Angel 3.0 《分布式机器学习：算法、理论与实践》刘铁岩等著 从学习目标的角度，机器学习可以大体分成回归、分类、排序、有结构预测等类别。这些类别的主要差别在于机器学习模型输出的格式，以及如何衡量输出的准确程度。 在回归问题里，模型的输出值一般是一个连续的标量，人们通常用模型输出与真值之间的最小平方误差来衡量模型的准确程度。 在分类问题里，模型的输出是一个或多个类别标签，人们通常使用0-1误差及其损失函数(如交叉嫡、Hinge函数、指数函数等)来衡量模型的准确程度。 在排序问题里，模型的输出是一个经过排序的对象列表，人们通常用序对级别(pairwise)或列表级别(listwise)的损失函数来衡量模型的准确程度。 在更加通用的有结构预测问题中，则需要具体问题具体分析，利用领域知识定义合适的输出格式和模型准确程度的判别准则。 从训练数据特性的角度，机器学习可以大体分为有监督学习、半监督学习、无监督学习、弱监督学习等类别。 有监督学习，指的是每个训练数据都拥有标签。这样一来，在每个训练样本上都可以精准地计算损失，并且根据损失对模型进行优化。 半监督学习指的是训练集里同时存在有标签数据和无标签数据。通常人们需要对无标签数据进行一些预处理(比如根据它们和有标签数据的相似性来预测其伪标签，或者计算它们彼此之间的相似性以获取对整个数据集分布的先验知识)，然后利用它们来协助原有的训练过程(比如把伪标签当作真实标签使用，或把数据集分布作为正则项来增强模型的泛化能力)。 无监督学习处理的数据全都是无标签的。学习的目的是从数据中发掘关联规则，或者利用数据在输入空间中的相互关系(如相似性、距离、偏序关系等)对数据进行聚类和影响力排序。 弱监督学习中存在某种形式的奖励信号，该信号可以用于模型训练，但是没有样本标签那么直接、完全、确切或者准确。强化学习是一类典型的弱监督学习问题，它无须依赖预先给定的离线训练数据，而是通过与环境的试探性交互来进行学习。具体而言，学习机制通过选择并执行某些动作，导致环境状态变化，并得到来自环境的奖励信号。学习的目标是寻找一个合适的动作选择策略，使产生的动作序列获得最优的累计奖励。 从模型复杂程度的角度，机器学习可以分为线性模型与非线性模型(或浅层模型与深层模型)。 线性模型包括线性回归、逻辑回归、线性支持向量机等。这些模型可以通过核化进行非线性变换，从而获得更加强大的表达能力。 非线性模型包括决策树、深层神经网络(包括全连接神经网络、卷积神经网络、循环神经网络等)。它们具有很强的表达能力，可以更好地拟合训练数据。 从模型的功能角度，机器学习可以划分为生成模型和判别模型。 生成模型在学习过程中通常以最大化训练数据的似然为目的，关注的是输入样本和标签的联合概率分布。生成模型要学习的概率分布比较复杂，但适用场合很丰富，既可以用来完成分类任务，也可以实现概率密度估计或样本的随机生成。 判别模型通常最大化的是条件似然，也就是关注在给定输入样本的前提下标签的条件概率。判别模型单刀直入，解决的是一个判别问题，不需要对联合分布做不必要的刻画，学习效率比较高，但适用场景也因此受到一定程度的限制。 1、有监督学习：通过已有的训练样本去训练得到一个最优模型，再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现预测和分类的目的，也就具有了对未知数据进行预测和分类的能力。就如有标准答案的练习题，然后再去考试，相比没有答案的练习题然后去考试准确率更高。又如我们小的时候不知道牛和鸟是否属于一类，但当我们随着长大各种知识不断输入，我们脑中的模型越来越准确，判断动物也越来越准确。 有监督学习可分为回归和分类。 回归：即给出一堆自变量X和因变量Y，拟合出一个函数，这些自变量X就是特征向量，因变量Y就是标签。 而且标签的值连续的，例LR。 分类：其数据集，由特征向量X和它们的标签Y组成，当你利用数据训练出模型后，给你一个只知道特征向量不知道标签的数据，让你求它的标签是哪一个？其输出结果是离散的。例如logistics、SVM、KNN等。 2、无监督学习：我们事先没有任何训练样本，而需要直接对数据进行建模。比如我们去参观一个画展，我们完全对艺术一无所知，但是欣赏完多幅作品之后，我们也能把它们分成不同的派别。无监督学习主要算法是聚类，聚类目的在于把相似的东西聚在一起，主要通过计算样本间和群体间距离得到，主要算法包括Kmeans、层次聚类、EM算法。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/机器学习基础_距离.html":{"url":"chapters/机器学习基础_距离.html","title":"机器学习基础_距离","keywords":"","body":"距离闵科夫斯基距离曼哈顿距离(Manhattan)欧式距离(Euclidean )切比雪夫距离(Chebyshev)夹角余弦(Cosine)汉明距离(Hamming)杰卡德相似系数(Jaccard Similarity)相关系数和相关距离马氏距离 [TOC] 距离 范数：常被用来度量某个向量空间(或矩阵)中的每个向量的长度或大小。 满足一定的条件，即①非负性；②齐次性；③三角不等式。 (1) L1范数:||x||为x向量各个元素绝对值之和 (2) L2范数(Euclidean范数):||x||为x向量各个元素平方和的开方 (3) Lp范数:||x||为x向量各个元素绝对值p次方和的1/p次方 (4) Loo:||x||为x向量各个元素绝对值最大的那个元素 闵科夫斯基距离 严格意义上讲，阅可夫斯基距离不是一种距离，而是一组距离的定义。 两个n维变量A(x_{11},x_{12},...x_{1n})与B(x_{21},x_{22},...x_{2n})间的闵科夫斯基距离定义为: \td_{12}=\\sqrt[p]{\\sum_{k=1}^{n}{(x_{1k}-x_{2k})^p}} 其中p是一个变参数。 当p=1时，就是曼哈顿距离 当p=2时，就是欧式距离 当p=3时，就是切比雪夫距离 根据变参数的不同，闵可夫斯基距离可以表示一类的距离。 曼哈顿距离(Manhattan) 实际驾驶距离就是这个“曼哈顿距离”（L1范数），而这也是曼哈顿距离名称的来源。 曼哈顿距离也称为城市街区距离（City Block distance)，如图所示。 (a) 二维平面两点A(x_1,y_1)与B(x_2,y_2)间的曼哈顿距离： > d_{12} = |x_1-x_2| + |y_1-y_2| > (b) 两个n维向量A(x_{11},x_{12},...x_{1n})与B(x_{21},x_{22},...x_{2n})间的曼哈顿距离： > d_{12}=\\sum_{k=1}^{n}{|x_{1k}-x_{2k}|} > 欧式距离(Euclidean ) 欧氏距离（L2范数)是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式，如图所示: (a) 二维平面上两点a(x_1,y_1)与b(x_2,y_2)间的欧氏距离： > d_{12}=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2} > (b) 三维空间两点A(x_1,y_1,z_1)与B(x_2,y_2,z_2)间的欧氏距离： > d_{12}=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2(z_1-z_2)^2} > (c) 两个n维向量A(x_{11},x_{12},...,x_{1n})与B(x_{21},x_{22},...,x_{2n})间的欧氏距离： > d_{12}=\\sqrt{\\sum_{k=1}^{n}{(x_{1k}-x_{2k})^2}} > 表示为向量运算的形式： > d_{12}=\\sqrt{(A-B)(A-B)^T} > 切比雪夫距离(Chebyshev) 国际象棋玩过吗？国王走一步能够移动到相邻的8个方格中的任意一个。 那么国王从格子x_1,y_1走到格子x_2,y_2最少需要多少步？ 自己走走试试。你会发现最少步数总是max(|x_2-x_1|,|y_2-y_1|)步。 有一种类似的距离度量方法叫作切比雪夫距离(Loo范数)。 （1）二维平面两点A(x_1,y_1)与B(x_2,y_2)间的切比雪夫距离： > d_{12}=max(|x_1-x_2|,|y_1-y_2|) > （2) 两个n维向量A(x_{11},x_{12},...,x_{1n})与B(x_{21},x_{22},...,x_{2n})间的切比雪夫距离： > d_{12}=\\underset{i}{max}{(|x_{1i}-x_{2i}|)} > 这个公式的另一种等价形式是： > d_{12}=\\lim_{k \\to \\infty}{(\\sum_{i=1}^{n}{|x_{1i}-x_{2i}|^k)^{1/k}}} > 夹角余弦(Cosine) 几何中的夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡 量样本向量之间的差异。 （1）在二维空间中向量A(x_1,y_1)与向量B(x_2,y_2)的夹角余弦公式： > cos\\theta=\\frac{x_1x_2+y_1y_2}{\\sqrt{x_1^2+y_1^2}{\\sqrt{x_2^2+y_x^2}}} > （2）两个n维样本点A(x_{11},x_{12},...x_{1n})与B(x_{21},x_{22},...,x_{2n})的夹角余弦：类似地，对于两个n维样本点A(x_{11},x_{12},...x_{1n})与B(x_{21},x_{22},...,x_{2n})，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。 > cos\\theta=\\frac{AB}{|A||B|} > 即： > cos\\theta=\\frac{\\sum_{k=1}^{n}{x_{1k}x_{2k}}}{\\sqrt{\\sum_{k=1}^{n}{x_{1k}^2}{\\sqrt{ \\sum_{k=1}^{n}{x_{2k}^2} }}}} > 夹角余弦取值范围为[-1,1]。 夹角余弦越大，表示两个向量的夹角越小；夹角余弦越 小，表示两个向量的夹角越大。 当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1。 汉明距离(Hamming) （1）汉明距离的定义： 两个等长字符串s1与s2之间的汉明距离定义为将其中一个 变为另外一个所需要的最小替换次数。 例如： 1111 与 1001 之间的汉明距离为 2。 1011101 与 1001001 之间的汉明距离是 2。 2143896 与 2233796 之间的汉明距离是 3。 \"toned\" 与 \"roses\" 之间的汉明距离是 3。 应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。 杰卡德相似系数(Jaccard Similarity) （1）杰卡德相似系数：两个集合A和B的交集元素在A、B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。 >J(A,B)=\\frac{|A \\cap B|}{|A \\cup B|} > 杰卡德相似系数是衡量两个集合的相似度的一种指标。 （2）杰卡德距离：与杰卡德相似系数相反的概念是杰卡德距离(Jaccard Distance)。 杰卡德距离可用如下公式表示： >J_{\\delta}{(A,B)}=1-J(A,B)=\\frac{|A \\cup B|-|A \\cap B|}{|A \\cup B|} > 杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。 （3）杰卡德相似系数与杰卡德距离的应用。 可将杰卡德相似系数用在衡量样本的相似度上。 样本A与样本B是两个n维向量，而且所有维度的取值都是0或1。 ​ 例如，A(0111) 和B(1011)。我们将样本看成一个集合，1表示集合包含该元素，0表示集合不包含该 元素。 相关系数和相关距离 （1）相关系数的定义： >\\rho_{XY}=\\frac{Cov(X,Y)}{\\sqrt{D(X)}\\sqrt{D(Y)}}=\\frac{E((X-EX)(Y-EY))}{\\sqrt{D(X)}\\sqrt{D(Y)}} > ​ 相关系数是衡量两个特征列之间相关程度的一种方法，其取值范围是[-1,1]。 ​ 相关系数的绝对值越大，表明特征列X与Y的相关度越高。 ​ 当X与Y线性相关时，相关系数取值为1(正线性相关)或-1(负线性相关)。 （2）相关距离的定义： >D_{XY}=1-\\rho_{XY} > 马氏距离 （1）马氏距离的定义：有M个样本向量X~X_m，协方差矩阵记为S，均值记为向量\\mu，则其中样本向量X到\\mu的马氏距离表示为： >D(X)=\\sqrt{(X-\\mu)^TS^{-1}(X-\\mu)} > 而其中向量X_i，与X_j之间的马氏距离定义为： >D(X_i,X_j)=\\sqrt{(X_i-X_j)^{T}S^{-1}(X_i-X_j)} > 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布），则公式变成了欧氏距离公式： >D(X_i,X_j)=\\sqrt{(X_i-X_j)^{T}(X_i,X_j)} > 若协方差矩阵是对角矩阵，则公式变成了标准化欧氏距离公式。 （2）马氏距离的优点：量纲无关，排除变量之间的相关性的干扰。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/机器学习基础_概率论基础.html":{"url":"chapters/机器学习基础_概率论基础.html","title":"机器学习基础_概率论基础","keywords":"","body":"概率论基础概率空间随机变量概率分布、联合分布和边缘分布条件分布条件独立链式法则和贝叶斯定理离散分布和连续分布期望方差伯努利、泊松、高斯分布Jenson不等式 [TOC] 概率论基础 概率空间 说到概率，通常是指一个具有不确定性的event发生的可能性，例如，下周二下雨的概率。 因此，为了正式地讨论概率论，我们首先要明确什么是可能事件。 正规说来，一个probability space是由三元组$(\\Omega,F,P)$定义： ​ -$\\Omega$为样本空间 ​ -$F \\subseteq 2^\\Omega$(0的幂集)为(可度量的)事件空间 ​ -$P$为将事件$E \\in F$映射到$0~1$真值区间的概率度量(概率分布)，可以将$P$看作概率函数 注：$\\Omega$的幂集$2^\\Omega$--是$ \\Omega $的所有子集的集合，符号：$P(\\Omega)={ U|U \\subseteq \\Omega}$，$| \\Omega |=n$个元素，$|P( \\Omega)|=2^n$个元素。 假设给定样本空间$ \\Omega $，则对于事件空间$F$来说： ​ -$F$包含$ \\Omega $本身和$\\emptyset$ ​ -$F$对于并集闭合，例如：如果$\\alpha , \\beta \\in F$，则$\\alpha \\cup \\beta \\in F$ ​ -$F$对于补集闭合，例如：如果$\\alpha \\in F$，则$(\\Omega / \\alpha) \\in F$ Example1: 假如我们投掷一个（6面）骰子，那么可能的样本空间$\\Omega ={ 1,2,3,4,5,6}$。 我们可能感兴趣的事件是骰子点数是奇数还是偶数， ​ 那么这种情况下事件空间就是$F={ \\emptyset, {1,3,5}, {2,4,6} }$. ​ 可以看到样本空间$ \\Omega $为有限集时，就像上一个例子，我们通常令事件空间$F$为$2^{\\Omega}$。这种策略并不完全通用，但是在实际使用中通常是有效的。 ​ 然而，当样本空间为无限集时，我们需要仔细定义事件空间。给定一个事件空间$F$，概率函数$P$需要满足几个公理： ​ -(非负)对于所有$\\alpha$，$P(a）\\ge 0$ ​ -P(F)=1，事件空间的概率值为1 ​ -(互斥事件的加法法则)对于所有$\\alpha,\\beta \\in F$和$\\alpha \\cap \\beta = \\emptyset$，$P(\\alpha \\cup \\beta)=P(\\alpha)+P(\\beta)$ Example2: 回到掷骰子的例子，假设事件空间$F$为$2^\\Omega$，进一步地，定义$F$上的概率函数$P$为： >P(\\{1 \\})=P(\\{2 \\})=...=P(\\{6 \\})=\\frac{1}{6} > ​ 那么这种概率分布$P$可以完整定义任意给出事件的发生概率(通过可加性公理)。 ​ 例如，投掷点数为偶数的概率为： $P{2,4,6}=P{2}+P{4}+P{6}=\\frac{1}{6}+\\frac{1}{6}+\\frac{1}{6}=\\frac{1}{62}$ ​ 因为任意事件(此处指样本空间内的投掷出各点数)之间都没有交集 随机变量 随机变量在概率论中扮演着一个重要角色。 最重要的一个事实是，随机变量并不是变量，它们实际上是将(样本空间中的)结果映射到真值的函数。 我们通常用一个大写字母来表示随机变量。 Example3: 还是以掷骰子为例， ​ $X$为取决于投掷结果的随机变量，$X$的一个自然选择是将 $i$ 映射到值 $i$。 ​ 例如，将事件“投掷1点“映射到值1。我们也可以选择一些特别的映射，例如，我们有一个随机变量$Y$--将所有的结果映射到0，这就是一个很无聊的函数。或者随机变量$Z$--当 $i$ 为奇数时，将结果映射到$2^i$；当 $i$ 为偶数时，将结果 $i$ 映射到 $i$。 从某种意义上说，随机变量可以将事件空间的形式概念抽象出来，通过定义随机变量来采集相关事件。 举个例子，考虑Example1中投掷点数为奇/偶的事件空间。 我们其实可以定义一个随机变量，当结果 $i$ 为奇数时取值为1，否则随机变量取值为0。 这种二元算计变量在实际中非常常见，通常以指示变量为人所知，它是因用于指示某一特定事件是否发生而得名。 所以为什么我们要引进事件空间？就是因为当一个人在学习概率论(更严格来说)通过计量理论来学习时，样本空间和事件空间的区别非常重要。 ​ 随机变量让我们能提供一种对于概率论的更加统一的处理方式。取值为$a$的随机变量$X$的概率可以记为： $P(X=a)$或$P_X(a)$ 概率分布、联合分布和边缘分布 变量的分布，正式来说，它是指一个随机变量取某一特定值的概率， Example4: 假设在投掷一个骰子的样本空间$\\Omega$上定义一个随机变量$X$，如果骰子是均匀的，则$X$的分布为：$P_x(1)=P_x(2)=...=P_x(6)=\\frac{1}{6}$。 ​ 注意，尽管这个例子和Example2类似，但是它们有着不同的语义。 ​ Example2中定义的概率分布是对于事件而言，而这个例子中是随机变量的概率分布。 ​ 我们用$P(X)$来表示随机变量$X$的概率分布。 有时候，我们会同时讨论大于一个变量的概率分布，这种概率分布称为联合分布，因为此事的概率是由所涉及到的所有变量共同决定的。 Example5: 在投掷一个骰子的样本空间上定义一个随机变量$X$。 定义一个指示变量$Y$，当抛硬币结果为正面朝上时取1，反面朝上时取 0。 假设骰子和硬币都是均匀的，则$X$和$Y$的联合分布如下： $P$ $X=1$ $X=2$ $X=3$ $X=4$ $X=5 $ $X=6$ $Y=0$ $\\frac{1}{12}$ $\\frac{1}{12}$ $\\frac{1}{12}$ $\\frac{1}{12}$ $\\frac{1}{12}$ $\\frac{1}{12}$ $Y=1$ $\\frac{1}{12}$ $\\frac{1}{12}$ $\\frac{1}{12}$ $\\frac{1}{12}$ $\\frac{1}{12}$ $\\frac{1}{12}$ 像前面一样，我们可以用$P(X=a,Y=b)$或$P_{x,y}(a,b)$来表示$X$取值为$a$且$Y$取值为$b$时的概率。 用$P(X,Y)$来表示它们的联合分布。 假定有一个随机变量$X$和$Y$的联合分布，我们就能讨论$X$或$Y$的边缘分布。 边缘分布是指一个随机变量对于其自身的概率分布。 为了得到 一个随机变量的边缘分布，我们将该分布中的所有其它变量相加，准确来说，就是： >P(X)=\\sum_{b \\in Val(Y)}{P（X,Y= b）} > 之所以取名为边缘分布，是因为如果我们将一个联合分布的一列(或一行)的输入相加，将结果写在它的最后(也就是边缘)，那么该结果就是这个随机变量取该值时的概率。 当然，这种思路仅在联合分布涉及两个变量时有帮助。 条件分布 条件分布为概率论中用于探讨不确定性的关键工具之一。 它明确了在另一随机变量已知的情况下（或者更通俗来说，当已知某事件为真 时）的某一随机变量的分布。 正式地，给定$Y=b$时，$X=a$的条件概率定义为： >P(X=a|Y=b)=\\frac{P(X=a,Y=b)}{P(Y=b)} > 注意，当$Y=b$的概率为0时，上式不成立。 Example6: 假设我们已知一个骰子投出的点数为奇数，想要知道投出的点数为\"1\"的概率。 令$X$为代表点数的随机变量，$Y$为指示变量，当点数为奇数时取值为1，那么我们期望的概率可以写为： >P(X=1|Y=1)=\\frac{P(X=1,Y=1)}{P(Y=1)}=\\frac{\\frac{1}{6}}{\\frac{1}{2}}=\\frac{1}{3} > 条件概率的思想可以自然地扩展到一个随机变量的分布是以多个变量为条件时，即： >P(X=a|Y=b,Z=c)=\\frac{P(X=a,Y=b,Z=c)}{P(Y=b,Z=c)} > 我们用$P(X|Y = b)$来表示当$Y=b$时随机变量$X$的分布，也可以用$P(XIY)$来表示$X$的一系列分布，其中每一个都对应不同的$Y$可以取的值。 条件独立 在概率论中，独立性是指随机变量的分布不因知道其它随机变量的值而改变。 ​ 在机器学习中，我们通常都会对数据做这样的假设。例如，我们会假设训练样本是从某一底层空间独立提取；并且假设样例的标签独立于样例$j(i \\neq j)$的特性。 ​ 从数学角度来说，随机变量$X$独立于$Y$，当$P(X)=P(X|Y)$ (注意，上式没有标明$X$，$Y$的取值，也就是说该公式对任意$X$，$Y$可能的取值均成立) ​ 利用等式(2)，很容易可以证明如果$X$对$Y$独立，那么$Y$也独立于$X$。当$X$和$Y$相互独立时，记为$X \\bot Y$。 对于随机变量$X$和$Y$的独立性，有一个等价的数学公式： >P(X,Y)=P(X)P(Y)​ > 我们有时也会讨论条件独立，就是当我们当我们知道一个随机变量（或者更一般地，一组随机变量）的值时，那么其它随机变量之间相互独立。 正式地，我 们说\"给定$Z$，$X$和$Y$条件独立\"，如果： >P(X|Z)=P(X|Y,Z) > 或者等价的： >P(X,Y|Z)=P(X|Z)P(Y|Z) > 机器学习(Andrew Ng)的课中会有一个朴素贝叶斯假设就是条件独立的一个例子。 该学习算法对内容做出假设，用未分辨电子邮件是否为垃圾邮件。假设无论邮件是否为垃圾邮件，单词$x$出现在邮件中的概率条件独立于单词$y$。 很明显这个假设不是不失一般性的，因为某些单词几乎总是同时出现。然而最终结果是，这个简单的假设对结果的影响并不大，且无论如何都可以让我们快速判别垃圾邮件。 链式法则和贝叶斯定理 我们现在给出两个与联合分布和条件分布相关的，基础但是重要的可操作定理。 第一个叫做链式法则，它可以看做等式(2)对于多变量的一般形式。 定理1（链式法则）： >P(X_1,X_2,...,X_n)=P(X_1)P(X_2|X_1)...P(X_n|X_1,X_2,...X_{n-1}) > 链式法则通常用于计算多个随机变量的联合概率，特别是在变量之间相互为(条件)独立时会非常有用。 注意，在使用链式法则时，我们可以选择展开随机变量的顺序；选择正确的顺序通常可以让概率的计算变得更加简单。 第二个要介绍的是贝叶斯定理。 利用贝叶斯定理，我们可以通过条件概率$P(Y|X)$算出$P(XIY)$，从某种意义上说就是交换条件。 它也可以通过等式(2)推导出。 定理2（贝叶斯定理）： >P(X|Y)=\\frac{P(Y|X)P(X)}{P(Y)} > 记得，如果$P(Y)$没有给出，我们可以用等式(1)找到它： >P(Y)=\\sum_{a \\in Val(X)}{P(X=a,Y)}=\\sum_{a \\in Val(X)}{P(Y|X=a)P(X=a)} > 这种等式(1)的应用有时也被称为全概率公式，贝叶斯定理可以推广到多个随机变量的情况。 在有疑问的时候，我们都可以参考条件概率的定义方式，弄清楚其细节。 Example7: 考虑以下的条件概率：$P(X,Y|Z)$和$(X|Y,Z)$ >P(X,Y|Z)=\\frac{P(Z|X,Y)P(X,Y)}{P(Z)}=\\frac{P(Y,Z|X)P(X)}{P(Z)} > >P(X|Y,Z)=\\frac{P(Y|X,Z)P(X,Z)}{P(Y,Z)}=\\frac{P(Y|X,Z)P(X|Z)P(Z)}{P(Y|Z)P(Z)}=\\frac{P(Y|X,Z)P(X|Z)}{P(Y|Z)} > 离散分布和连续分布 离散分布:概率质量函数: 就一个离散分布而言，我们是指这种基本分布的随机变量只能取有限多个不同的值（或者样本空间有限）。 在定义一个离散分布时，我们可以简单地列举出随机变量取每一个可能值的概率。 这种列举方式称为概率质量函数（probability mass function[PMF])，因为它将（总概率的）每一个单元块分开，并将它们和随机变量可以取的不同值对应起来。这个可以类似的扩展到联合分布和条件分布。 连续分布:概率密度函数: 对连续分布而言，我们是指这种基本分布的随机变量能取无限多个不同值（或者说样本空间是无限的）。 连续分布相比离散分布来说是一种更加需要揣摩的情况，因为如果我们将每一个值取非零质量数，那么总质量相加就会是一个无限值，这样就不符合总概率相加等于1的要求。 ​ 在定义一个连续分布时，我们会使用概率密度函数（probability density function[PDF]）。 概率密度函数f是一个非负，可积(分)的函数，类似于： >\\int _{Val(X)}{f(x)dx}=1 > 符合PDF $f$ 的随机变量$X$的概率分布可以用如下公式计算： >P(a \\leq X \\leq b )= \\int _{a}^{b}{f(x)dx} > 注意，特别地，默认连续分布的随机变量取任意单一值的概率为零。 Example8：(均匀分布）假设随机变量$X$在$[0,1]$上均匀分布，则对应的PDF为： >f(x)=\\left\\{ >\\begin{aligned} >1 \\quad {if0 \\leq x \\leq b}\\\\ >0 \\quad {otherwise} >\\end{aligned} >\\right. > 我们可以确定$ \\int_{0}^{1}{dx}$为1，因此$f$为PDF。计算$X$的概率小于$1/2$. >P(X \\leq \\frac{1}{2})=\\int _{0}^{\\frac{1}{2}}{1dx}={|x|}_0^{\\frac{1}{2}}=\\frac{1}{2} > 更一般地，假设$X$在$[a,b]$上均匀分布，那么PDF即为： >f(x)=\\left\\{ >\\begin{aligned} >\\frac{1}{b-a} \\quad {if0 \\leq x \\leq b}\\\\ >0 \\quad {otherwise} >\\end{aligned} >\\right. > 有时我们也会讨论累积分布函数，这种函数给出了随机变量在小于某一值的概率。 累积分布函数$F$和基本概率密度函数$f$的关系如下： >F(b)=P(X \\leq b)= \\int_{-\\infty}^{b}{f(x)dx} > 因此，$F(x)= \\int f(x)dx$(就不定积分而言）。 要将连续分布的定义扩展到联合分布，需要把概率密度函数扩展为多个参数，即： >P(a_1 \\leq X_1 \\leq b_1, a_2 \\leq X_2 \\leq b_2,...a_n \\leq X_n \\leq b_n)=\\int_{a_1}^{b_1}{\\int _{a_2}^{b_2}{... \\int _{a_n}^{b_n}{f(x_1,x_2,...,x_n)d_{x1}d_{x2}...d_{xn}}}} > 将条件分布扩展到连续随机变量时，会遇到一个问题——连续随机变量在单个值上的概率为0，因此等式(2)不成立，因为分母等于0。 为了定义连续变量的条件分布，要令$f(x,y)$为$X$和$Y$的联合分布。 通过分析，我们能看到基于分布$P(Y|X)$的PDF $f(y|x)$ 为： >f(y|x)=\\frac{f(x,y)}{f(x)} > 即如果直接用$P$的话，$P$可能在分母为零，所以用$f$，通过$f$积分间接得到$P$，例如： >P(a \\leq Y \\leq b |X=c)=\\int_{a}^{b}{f(y|c)dy}=\\int_{a}^{b}{\\frac{f(c,y)}{f(c)}dy} > 期望 我们对随机变量做的最常见的操作之一就是计算它的期望，也就是它的平均值(mean)，期望值(expected value)，或一阶矩(first moment)。 随机变量的期望记为$E(x)$，计算公式： >E(x)=\\sum_{a \\in Val(x)}{aP(X=a)}或E(x)=\\int_{a \\in Val(x)}{xf(x)dx} > Example9: 令$X$为投掷一个均匀散子的结果，则$X$的期望为： >E(X)=(1)\\frac{1}{6}+(2)\\frac{1}{6}+...+(1)\\frac{6}{6}=3\\frac{1}{2} > ​ 有时我们可能会对计算随机变量$X$的某一函数f的期望值$f$感兴趣，再次重申，随机变量本身也是一个函数，因此最简单的考虑方法是定义一个新的随机变量$Y=f(X)$，然后计算$Y$的期望。 当使用指示变量时，一个有用的判别方式是： >E(X)=P(X=1), 其中X为指示变量 > 此处可以脑补$X$还有一个取值为0，即$E(x)=1 \\times P(X=1)+0 \\times P(X=0)=P(X=1)$ 当遇到随机变量的和时，一个最重要的规则之一是线性期望(linearity of expectations)。 定理3(线性期望): 令$X_1,X_2,...,X_n$为(可能是独立的)随机变量： >E(X_1,X_2,...+X_n)=E(X_1)+E(x_2)+...+E(X_n) > 期望为线性函数。 期望的线性非常强大，因为它对于变量是否独立没有限利。当我们对随机变显的结果进行处理时，通常没什么可说的，但是，当随机变量相互独立时有 定理4：令$X$和$Y$为相互独立的随机变量，则： >E(XY)=E(X)E(Y) > 方差 一个随机变显的方差描述的是它的商散程度，也就是该变量离其期望值的距离。 一个实随机变量的方差也称为它的二阶矩或二阶中心动差，恰巧也是它的二阶累积量，方差的算术平方根称为该随机变量的标准差。 方差的定义： > Var(X)=E((X-E(X))^2) > 随机变量的方差通常记为$\\sigma ^2$，给它取平方的原因是因为我们通常想要找到$ \\sigma$，也就是标准差。 方差和标准差可以用公式$\\sigma=\\sqrt{Var(X)}$相关联。 为了找到随机变量$X$的方差，通常用以下替代公式更简单： > Var(X)=E(X^2)-(E(X))^2 > 注意，不同于期望|方差不是关于随机变量$X$的线性函数，事实上，我们可以证明$(aX+b)$的方差为： > Var(aX+b)=a^2Var(X) > 如果随机变量$X$和$Y$相互独立，那么： > Var(X+Y)=Var(X)Var(Y),如果X \\bot Y > 有时我们也会讨论两个随机变量的协方差，它可以用来威量两个随机变量的相关性，定义如下： > Cov(X,Y)=E((X-E(X))(Y-E(Y))) > 伯努利、泊松、高斯分布 伯努利(Bernoulli)分布: 伯努利分布是最基础的概率分布之一，一个服从伯努利分布的随机变量有两种取值${ 0,1}$，它能通过一个变量$p$未表示其概率，为了方便我们令$P(X=1)$为$p$。 它通常用于预测试验是否成功。有时将一个服从伯努利分布的变量$X$的概率分布按如下表示会很有用： > P(X)=p^x(1-p)^{1-x} > 一个伯努利分布起作用的例子是Lecture Notes1中的分类任务。 为了给这个任务开发一个逻辑回归算法，对于特征来说，我们假设标签遵循伯努利概率分布。 泊松(Poisson)分布: 泊松分布是一种非常有用的概率分布，通常用于处理事件发生次数的概率分布。 在给定一个事件发生的固定平均概率，并且在该段事件内事件发生相互独立时，它可以用来度量单位间内事件发生的次数，它包含一个参数——平均事件发生率入。 泊松分布的概率质量函数为： > P(X=k)=\\frac{exp(-\\lambda )\\lambda^k}{k!} > 服从泊松分布的随机变量的平均值为$\\lambda$，其方差也为$\\lambda$，$E(X)=V(X)=\\lambda$ 高斯(Gaussian)分布: 高斯分布也就是正态分布，是概率论中最”通用”的概率分布之一，并且在很多环境中都有出现。 例如，在试验数量很大时用在二项分布的近似处理中，或者在平均事件发生率很高时用于泊松分布。 它还和大数定理相关。对于很多问题来说，我们还会经常假设系统中的噪声服从高斯分布。 上图为不同期望和方差下的高斯分布。 高斯分布由两个参数决定：期望$\\mu$和方差$\\sigma ^2$。 其概率密度函数为： > f(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma}}exp(-\\frac{(x-\\mu)^2}{2 \\sigma ^2}) > 为了更好的感受概率分布随着期望和方差的改变，在上图中绘利了三种不同的高斯分布。 一个$k$维多变量高斯分布用参数$(\\mu,\\sum )$表示，其中，$\\mu$为$ \\mathbb{R} ^k$上的期望矢量，$\\sum$为$\\mathbb{R} ^{k \\times k}$上的协方差矩阵， 也就是说$\\sum{ii}=Var(X_i)$且$\\sum{ij}=Cov(X_i,Y_j)$。 其概率密度函数由输入的矢量定义： > f(x)=\\frac{1}{\\sqrt{2 \\pi ^k |\\sum|}}exp(-\\frac{1}{2}(x-\\mu)^T \\sum^{-1}{x-\\mu}) > (我们标记矩阵$A$的行列式为$|A|$，其转置为$A^{-1}$） Jenson不等式 有时我们会计算一个函数对某个随机变量的期望，通常我们只需要一个区间而不是具体的某个值。在这种情况下，如果该函数是凸函数或 者凹函数，通过Jenson不等式，我们可以通过计算随机变量自身期望处的函数值来获得一个区间。 (上图为Jenson不等式图示) 定理5(Jenson不等式): 令$X$为一个随机变量，$f$为凸函数，那么： >F(E(X)) \\leq E(f(X)) > 如果$f$为凹函数，那么： >f(E(X)) \\geq E(f(X)) > 尽管我们可以用代数表示Jenson不等式，但是通过一张图更容易理解。 上图中的函数为一个凹函数，我们可以看到该函数任意两点之间的直线都在函数的上方，也就是说，如果一个随机变量只能取两个值，那么Jenson不等式成立。这个也可以比较直接地推广到一般随机变量。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/机器学习基础_线性代数基础.html":{"url":"chapters/机器学习基础_线性代数基础.html","title":"机器学习基础_线性代数基础","keywords":"","body":"线性代数基础 [TOC] 线性代数基础 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/机器学习基础_微积分基础.html":{"url":"chapters/机器学习基础_微积分基础.html","title":"机器学习基础_微积分基础","keywords":"","body":"微积分基础 [TOC] 微积分基础 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/机器学习基础_最优化理论.html":{"url":"chapters/机器学习基础_最优化理论.html","title":"机器学习基础_最优化理论","keywords":"","body":"最优化理论常见优化方法 [TOC] 最优化理论 常见优化方法 前文提到了机器学习领域常用的损失函数以及预测模型的结构。在此基础上，我们需要采用有效的优化手段，通过最小化经验损失函数来求出预测模型里的参数。 最优化这个领域历史悠久，远在机器学习兴起之前，梯度下降法[45]就已经被提出，后续的共扼梯度法6]、坐标下降法[71、牛顿法]、拟牛顿法）、Frank-Wolfe方法[501、Nesterov加速方法51]、内点法2]、对偶方法58]等其他确定性优化算法也被陆续发明。随着大数据的兴起，为了减小优化过程中每次迭代的计算复杂度，人们开始关注随机优化算法，比如随机梯度下降法、随机坐标下降法等。近年来由于深层神经网络变得越来越重要，又有一些专门针对深层神经网络的优化算法被发明。由于我们将会用第4章、第5章两章篇幅对优化算法进行详细的介绍，因此本节就不再赘述，而是仅仅简单梳理一下优化算法的发展脉络。表2.2总结了典型的优化方法。 一阶算法 二阶算法 确定性算法 梯度下降法投影次梯度下降近端梯度下降法Frank-Wolfe算法Nesterov加速方法坐标下降法对偶坐标上升法 牛顿法拟牛顿法 随机算法 随机梯度下降法随机坐标下降法随机对偶坐标上升法随机方差减小梯度法 随机拟牛顿法 确定性优化方法从算法所使用的信息的角度可以分为一阶方法和二阶方法，从解问题的角度可以分为原始方法和对偶方法。所谓一阶方法指的是在优化过程中只利用了目 标函数的一阶导数信息，而二阶方法则要利用到目标函数的二阶导数（例如海森矩阵）。 所谓原始方法指的是针对原始问题描述中的变量进行优化，而对偶方法则是先通过对偶变换把原始问题转换成对偶问题，再针对对偶变量进行优化。 随着大数据的出现，确定性优化方法的效率成为瓶颈。于是，人们基于确定性优化方法设计了各种随机优化方法。这些算法的基本思想是每次迭代不使用全部训练样本或全部特征维度进行优化，而是随机抽样一个/一组样本，或一个/一组特征，再利用这些抽样的信息来计算一阶或二阶导数，对目标函数进行优化。在很多情况下，可以证明随机优化算法可以看作对原确定性算法的无偏意义上的实现，因此有一定的理论保障。但是，有时随机采样会带来比较大的方差，这就需要我们使用一些新的技术手段去控制方差（例如SVRG方法[5]等）。 上面提到的绝大部分优化算法在求解凸优化问题时的理论性质是比较清楚的，但是近年来随着深层神经网络变得越来越重要，非凸优化的问题逐渐浮出水面并受到大家的广泛重视。以上算法在非凸优化的情况下，还存在很大的理论空白。例如： ·这些优化算法的收敛性质在非凸问题上能否得以保持?与其在凸问题上的收敛速度有何不同? ·非凸问题有很多局部极值点和鞍点，如何通过初始化或者其他手段来逃离鞍点并且使最终的优化结果收敛到较好的极值点（相应的目标函数值低、鲁棒性强 或泛化性能好）？ ·已有的优化算法中哪些更适合用来训练神经网络?能否设计新的适用于神经网络的优化方法? 为了解决以上问题，尤其是加速神经网络训练的收敛速度，近年来人们提出了一系列针对神经网络优化的算法，例如：带冲量的随机梯度下降法[5]、Nesterov加速方法[581、AdaGrad 561、RMSProp 571、AdaDeltal58]、Adam1591、AMSGrad60l、等级优化算法[0]以及基于嫡的随机梯度下降法]等。从某种意义上讲，深度学习开启了非凸优化的大门，给拥有几百年历史的优化领域注入了新的活力。我们相信将来会有更多、更好的优化算法被提出，从而更快、更优地对复杂的非凸优化问题进行求解。 > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/机器学习基础_损失函数.html":{"url":"chapters/机器学习基础_损失函数.html","title":"机器学习基础_损失函数","keywords":"","body":"损失函数经验误差风险损失函数定义模型泛化误差 损失函数 经验误差风险 假定我们的目的是学习一个模型，用以自动判断某产品评论是正面的还是负面的。 为了更好地对机器学习的基本流程进行描述，我们首先对有监督的二分类问题进行数学建模。假设我们有一个训练数据集，包含n个样本\\{x_i\\}_{i=1}^n，每个样本x；可以表示成一个d维的向量：x_i \\in X \\subseteq R^d。样本x_{i}被赋予了一个标签y_i，表征该样本属于正类还是负类：y_i \\in Y= \\{+1,-1\\}。 假设我们最终想要通过机器学习获得一个分类模型g:X \\to R，它以X空间内任意的d维向量为输入，通过一个由参数w驱动的变换，输出一个分数，然后取这个分数的符号，得到Y空间的预测标签：sgn(g(x_i;w))。那么现在的问题是：什么样的分类模型才是好的？如何才能学到一个好的分类模型呢？ 可以根据一个分类模型g在训练集上的表现来评价它的好坏。换言之，我们把g作用在每一个训练样本x_i；上，获得相应的输出值g(x_i;w)，然后把这个输出值与x_i本身的类别标签y_i，进行比对，如果二者相同就说明g在这个样本上实现了正确的分类，否则就判定它分类错误。这个判定可以用一个简单的示性误差函数加以表示： >\\epsilon(w;x_i,y_i)=1_{|y_ig(x_i,w)| ​ 如果分类模型g把训练集里所有的样本或绝大部分样本都分到了正确的类别里，我们就说它是一个好的分类器；相反，如果g在很多样本上都做出了错误的判断，我们就说它不是一个好的分类器。这种定性的判断可以用一个称为经验误差风险的数值来进行定量衡量，也就是分类模型g在所有的训练样本上所犯错误的总和： >\\hat{\\epsilon}(w)=\\sum_{i=1}^{n}{1_{|y_ig(x_i,w)| ​ 如果\\hat{\\epsilon}(w)****为0或者取值很小，我们就说g的经验误差风险很小，是一个不错的分类模型。反之，如果\\hat{\\epsilon}(w)很大，则对应的经验误差风险很大，g就不是一个好的分类模型。 ​ 通常，我们会通过在训练集上最小化经验误差风险来训练分类模型。换言之，通过调节g的参数w，使得经验误差风险。\\hat{\\epsilon}(w)不断下降，最终达到最小值的时候，我们就获得了一个所谓“最优”的分类模型。这件事说起来容易，实操起来还是有难度的，主要的问题出在。\\hat{\\epsilon}(w)的数学性质上。按照上面的定义，\\hat{\\epsilon}(w)是一组示性函数的和，因此是一个不连续、不可导的函数，不易优化。 ​ 为了解决这个问题，人们提出了损失函数的概念。所谓的损失函数就是和误差函数有一定的关系(例如是误差函数的上界)但具有更好的数学性质(比如连续、可导、凸性等)，比较容易进行优化。通过对经验损失风险的最小化，我们可以间接地实现对经验误差风险\\hat{\\epsilon}(w)的最小化。为了便于引用，我们用\\hat{l}(w)来表示经验损失风险。 ​ 因为损失函数满足了连续可导的条件，所以在优化过程中选择面就比较宽了，有很多方法可供使用。我们既可以选择确定性的优化算法(包含以梯度下降法、坐标下降法为代表的一阶算法，以及以牛顿法、拟牛顿法为代表的二阶算法)，也可以选择随机性的优化算法(包括随机梯度下降法、随机坐标下降法、随机拟牛顿法等)。 ​ 当优化算法收敛以后，我们就得到了一个不错的模型。当然，这个“不错”的模型到底能有多好还要看损失函数的复杂程度。如果损失函数是个凸函数，则很容易通过上述方法找到全局最优模型；否则，多数情况下我们得到的只是局部最优模型。无论是哪种情况，未来我们将会使用这个学到的模型对未知的新样本进行分类。 损失函数定义 在二分类问题中，0-1误差是最终的评价准则，但是因为它不是一个连续的凸函数，直接用它来指导模型优化的过程未必是一个好的选择。为了解决这个问题，人们通常使用损失函数作为0-1误差的一个凸近似或者凸上界，然后通过最小化损失函数，来间接地达到最小化0-1误差的目的。本节将介绍几种典型的损失函数。 Hinge损失函数衡量的是预测模型的输出的符号和类别标签的符号是否一致以及一致的程度。 其具体数学形式如下： >l(w;x,y)=max\\{0,1-yg(x;w)\\} > ​ 从以上数学定义可以看出：当g(x;w)和y符号相同且乘积数值超过1时，损失函数取值为0；否则，将有一个线性的损失(二者符号不同时，乘积的绝对值越大，损失越大)。Hinge损失函数是一个连续凸函数，但是它在0点不可导，人们通常会选择次导数集合中的任意一个数值参与优化过程。我们从图2.3可以清晰地看出，Hinge损失是0-1误差的上界，因此通过最小化Hinge损失，可以有效地减小0-1误差，从而提高分类性能。 指数损失函数指数损失函数也是0-1误差的上界，它的具体形式如下（参见图2.4）： >l(x;x,y)=exp(-yg(x:w)) > 从以上定义可以看出，指数损失函数对于预测模型输出的符号与类别标签的符号不一致的情况有强烈的惩罚，相反，当二者符号一致且乘积数值较大时，损失函数的取值会非常小。指数损失函数的基本形状和Hinge损失函数很接近，只不过它对于符号不一致的情况的惩罚力度更大(指数力度vs.线性力度)，而且它是全程连续可导的凸函数，对于优化过程更加有利。 交叉熵损失函数也是常用的损失函数之一，它假设预测模型以下述形式决定了标签的概率分布： >P(Y=1|x;w)=\\frac{exp(g(x;w))}{exp(g(x;w))+exp(-g(x;w))} > 并且试图衡量该概率与标签之间的差别。其数学定义如下（参见图2.5）： >l(w;x,y)-\\sum_z \\in \\{-1,1 \\}{I_{|y=z|}}{}{logP(Y=z|x;w)} > 可见，最小化交叉熵损失函数等价于最大化预测函数g所对应的条件似然函数。 从以上定义可以看出，对于正类的样本而言，当12预测模型的输出接近于1时，损失很小；而当预测模型的输出接近于0时，则产生一个很大的损失。相反，对于负类的样本而言，当预测模型的输出接近于1时，会产生很大的损失；而当预测模型的输出接近于0时，则损失很小。交叉熵损失函数也是一个全程连续可导的凸函数，并且是0-1误差的上界。图2.5交叉熵损失函数以上介绍了一些常用的损失函数。虽然它们和0-1误差在形式上有所差别，但是从统计意义上讲，它们存在着很强的关联关系。可以证明，在一定假设下，以上损失函数对于0-1误差而言都具有统计一致性，也就是说，当样本趋近于无穷多的时候，按照最小化损失函数找到的最优模型也是在0-1误差意义下的最优模型。这就给使用这些损失函数奠定了理论基础。 模型泛化误差 机器学习算法的最终目标是最小化期望损失风险(也就是模型在任意未知测试样本上的表现)： >\\min_{g \\in G}L(g)=E_{x,y \\sim P_{x,y}}l(g;x,y) > 其中G是一个预先给定的函数族。 由于数据的真实分布P_{x,y}.通常是不知道的，我们的可用信息来自于训练数据S_n=\\{(x_1,y_1),...,(x_n,y_n)\\}。因此，我们的学习目标转化为最小化经验风险： >\\min_{g \\in G}{\\hat l(g)}=\\frac{1}{n} \\sum_{i=1}^{n}{l(g;x_i,y_i)} > 当函数空间G受限时，比如我们只允许优化算法在那些范数小于c的函数子空间里进行搜索，亦即G_c=\\{g:g \\in G, ||g||_G \\leq c\\}，我们称相应的学习问题为正则经验风险最小化。 优化算法对(正则化)经验风险最小化问题进行求解，并在算法结束的第T次迭代中输出模型\\hat g_r。我们希望所学习到的模型\\hat g_r的期望风险L(\\hat g_r)尽可能小，并将其定义为机器学习算法的泛化误差。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/特征工程_归一化.html":{"url":"chapters/特征工程_归一化.html","title":"特征工程_归一化","keywords":"","body":"归一化线性函数归一化零均值归一化其他归一化方式 [TOC] 归一化 为什么需要对数值型特征做归一化呢？ 我们不妨借助随机梯度下降的实例来说明归一化的重要性。假设有两种数值型特征，x_1的取值范围为[0,10]，x_2的取值范围为[0,3]，于是可以构造一个目标函数符合图1.1(a)中的等值图。 ​ 在学习速率相同的情况下，x_1的更新速度会大于x_2，需要较多的迭代才能找到最优解。 如果将x_1和x_2归一化到相同的数值区间后，优化目标的等值图会变成图1.1(b)中的圆形，x_1和x_2的更新速度变得更为一致，容易更快地通过梯度下降找到最优解。 当然，数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向 量机、神经网络等模型。 但对于决策树模型则并不适用，以C4.5为例， 决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比跟特征是否经过归一化是无关的， 因为归一化并不会改变样本在特征x上的信息增益。 线性函数归一化 线性函数归一化(Min-Max Scaling): 它对原始数据进行线性变换，使结果映射到[0,1]的范围，实现对原始数据的等比缩放。 >X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}} > 其中X为原始数据，X_{max}、X_{min}分别为数据最大值和最小值。 零均值归一化 零均值归一化(Z-Score Normalization): 将原始数据映射到均值为0、标准差为1的分布上。 具体来说，假设原始特征的均值为\\mu、标准差为\\sigma，那么归一化公式定义为 >z=\\frac{x-\\mu}{\\sigma} > 其他归一化方式 > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/特征工程_编码.html":{"url":"chapters/特征工程_编码.html","title":"特征工程_编码","keywords":"","body":"编码序号编码(标签编码)独热编码二进制编码其他编码 [TOC] 编码 序号编码(标签编码) 序号编码通常用于处理类别间具有大小关系的数据。 例如成绩，可以分为低、中、高三档，并且存在“高>中>低”的排序关系。 序号编码会按照大小关系对类别型特征赋予一个数值ID，例如高表示为3、 中表示为2、低表示为1，转换后依然保留了大小关系。 独热编码 独热编码通常用于处理类别间不具有大小关系的特征。 例如血型， 一共有4个取值（A型血、B型血、AB型血、O型血），独热编码会把血型变成一个4维稀疏向量 A型血表示为(1，0，0，0) B型血表示为(0，1，0，0) AB型表示为(0，0，1，0) O型血表示为(0，0， 0，1) 对于类别取值较多的情况下使用独热编码需要注意以下问题。 使用稀疏向量来节省空间。在独热编码下，特征向量只有某 一维取值为1，其他位置取值均为0。因此可以利用向量的稀疏表示有 效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。 配合特征选择来降低维度。高维度特征会带来几方面的问题。 在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量； 在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题； 通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度 二进制编码 二进制编码主要分为两步 先用序号编码给每个类别赋予一个类别 ID; 然后将类别ID对应的二进制编码作为结果 以A、B、AB、O 血型为例，表是二进制编码的过程。 血型 类别ID 二进制表示 独热编码 A 1 0 0 1 1 0 0 0 B 2 0 1 0 0 1 0 0 AB 3 0 1 1 0 0 1 0 O 4 1 0 0 0 0 0 1 A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为010；以此类推可以得到 AB型血和0型血的二进制表示。 可以看出，二进制编码本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。 其他编码 other: Helmert Contrast、Sum Constrast、Polynomial Constrast、Backward Difference Constrast Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/特征工程_特征组合.html":{"url":"chapters/特征工程_特征组合.html","title":"特征工程_特征组合","keywords":"","body":"特征组合 [TOC] 特征组合 > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/特征工程_特征选择.html":{"url":"chapters/特征工程_特征选择.html","title":"特征工程_特征选择","keywords":"","body":"特征选择 [TOC] 特征选择 > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/特征工程_文本表示模型.html":{"url":"chapters/特征工程_文本表示模型.html","title":"特征工程_文本表示模型","keywords":"","body":"文本表示模型词嵌入基于统计方法的词嵌入one hot vectorBag of WordsTF-IDF向量共现矩阵 (Cocurrence matrix)pmi计算语言模型分布式表示word2vecfasttextGloVeELMO网络模型过程效果 [TOC] BERT发展史（一）从词嵌入讲起 word2vec是如何得到词向量的？ NLP中的文本表示方法 文本表示模型 机器学习中，挖掘影响事件结果因素的过程称为特征工程，这一步的目的是依据现有数据挖掘出对结果有影响的因素（也叫特征），然后使用这些特征对结果进行建模。 好的特征对于建模的重要性。就像我们这世界有果皆有因，一件事情发生的背后，定有各种因素在背后支撑。我们希望计算机能够帮助我们建立各种因素取值和事件结果的对应关系，但在此之前，如何确定这些因素则是个让人头疼的东西。 词嵌入 NLP，就是使用计算机处理自然语言的过程。而我们都知道，计算机只能处理数值，因此自然语言需要以一定的形式转化为数值。词嵌入就是将词语（word）映射为数字的方式。一个单纯的实数包含的信息太少，一般我们映射为一个数值向量。这时我们会发现一个问题，怎么把词语转换为数值向量？自然语言本身蕴含了语义和句法等特征，如何在转换过程中保留这些抽象的特征？这点其实很重要，因为如果没有将自然语言的特征很好的保留下来，后续的所有工作就是对一些无意义的信息进行建模，自然得不到好的结果。纵观NLP的发展史，很多革命性的成果都是词嵌入的发展成果，如Word2Vec、ELMo和BERT，其实都是很好地将自然语言的特征在转换过程中进行了保留。 基于统计方法的词嵌入 one hot vector One-hot表示很容易理解。在一个语料库中，给每个字/词编码一个索引，根据索引进行one-hot表示。 John likes to watch movies. Mary likes too. John also likes to watch football games. 如果只需要表示出上面两句话中的单词，可以只对其中出现过的单词进行索引编码： {\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\":6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10} 其中的每个单词都可以用one-hot方法表示： John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] likes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] ... 当语料库非常大时，需要建立一个很大的字典对所有单词进行索引编码。比如100W个单词，每个单词就需要表示成100W维的向量，而且这个向量是很稀疏的，只有一个地方为1其他全为0。还有很重要的一点，这种表示方法无法表达单词与单词之间的相似程度，如beautiful和pretty可以表达相似的意思但是ont-hot法无法将之表示出来。 Bag of Words 词袋表示，也称为计数向量表示(Count Vectors)。文档的向量表示可以直接用单词的向量进行求和得到。 John likes to watch movies. Mary likes too. -->> [1, 2, 1, 1, 1, 0, 0, 0, 1, 1] John also likes to watch football games. -->> [1, 1, 1, 1, 0, 1, 1, 1, 0, 0] 横向来看我们把每条文本表示成了一个向量，纵向来看，不同文档中单词的个数又可以构成某个单词的词向量。如上文中的\"John\"纵向表示成[1,1]。 在真实世界中，我们的语料数据通常是包含数十万到上百万篇文档的，这些文档包含的非重复词会非常多。而每篇文档包含的词语数量相对于词典中的词语数量是非常小的，这会导致词频矩阵中会出现非常多的0。所以组织词典的另外一个方式是按词语出现的频率排序，选择频率最高的前1000个词语组成词典。 扩展：Bi-gram和N-gram 与词袋模型原理类似，Bi-gram将相邻两个单词编上索引，N-gram将相邻N个单词编上索引。 为 Bi-gram建立索引： {\"John likes”: 1, \"likes to”: 2, \"to watch”: 3, \"watch movies”: 4, \"Mary likes”: 5, \"likes too”: 6, \"John also”: 7, \"also likes”: 8, \"watch football\": 9, \"football games\": 10} 这样，原来的两句话就可以表示为： John likes to watch movies. Mary likes too. -->> [1, 1, 1, 1, 1, 1, 0, 0, 0, 0] John also likes to watch football games. -->> [0, 1, 1, 0, 0, 0, 1, 1, 1, 1] 这种做法的优点是考虑了词的顺序，但是缺点也很明显，就是造成了词向量的急剧膨胀。 词袋模型编码特点： 词袋模型是对文本（而不是字或词）进行编码； 编码后的向量长度是词典的长度； 该编码忽略词出现的次序； 在向量中，该单词的索引位置的值为单词在文本中出现的次数；如果索引位置的单词没有在文本中出现，则该值为 0 ； 缺点 该编码忽略词的位置信息，位置信息在文本中是一个很重要信息，词的位置不一样语义会有很大的差别（如 “猫爱吃老鼠” 和 “老鼠爱吃猫” 的编码一样）； 该编码方式虽然统计了词在文本中出现的次数，但仅仅通过“出现次数”这个属性无法区分常用词（如：“我”、“是”、“的”等）和关键词（如：“自然语言处理”、“NLP ”等）在文本中的重要程度； TF-IDF向量 我们有两篇文档D1和D2，每篇文章中所包含的词语及其词频如下图所示： 词语频率（Term Frequency）的定义如下： TF = (词语w在一篇文档中出现的次数)/（文档中的词语总数） 根据这个定义，可以计算TF(关于，D1) = 1/9；TF(学习，D2) = 3/9。 TF表示的是该词语对于该文档的贡献程度，我们认为与文章相关的词语出现的频率应该会比较高。 逆文档频率（Inverse Document Frequency）的定义如下： IDF = log（（文档的数量）/ （包含词语w的文档数量）） 根据这个定义，得到IDF(关于) = log(2/2) = 0；IDF(游戏) = log(2/1) = 0.301。 IDF想表达的是，如果一个词语在大部分甚至所有文档中都出现（词语中的中央空调），那么这个词语对于任意一篇文档都是不重要的。 最后，TF-IDF的计算方式就是把TF和IDF相乘即可。来看一下： TF-IDF(关于，D1) = （1/9）* 0 = 0 TF-IDF(游戏，D1) = （3/9） * 0.301 = 0.1003 TF-IDF(学习，D2) = （3/9）* 0.301 = 0.1003 这下就比较符合我们的直觉了：重要的词语（特征）应该具有更高的权重。 将词频矩阵中的词频值替换为TF-IDF值，我们就可以得到文档和词语的向量表示了。同时，由于TF-IDF的特征，TF-IDF还经常被用来提取文章中的关键词。 def def_tfidf(cls_info_lst: Dict): \"\"\" tf-idf普通实现 [06_TF-IDF算法代码示例](https://blog.csdn.net/u012990179/article/details/90338906) :param cls_info_lst: {\"A\":{\"a\":15,\"b\":18,...}, \"B\":{\"c\":48,...},...} :return: \"\"\" tf_dic = {} idf_res = defaultdict(int) for cls, cls_info in cls_info_lst.items(): total = sum(cls_info.values()) hy = {k: v / total for k, v in cls_info.items()} tf_dic[cls] = hy for k in set(cls_info.keys()): idf_res[k] += 1 total_doc = len(cls_info_lst) idf_res = {k: log(total_doc / (v + 1)) for k, v in idf_res.items()} cls_word_weights = [] for cls, tf_info in tf_dic.items(): t_lst = [] for k, v in tf_info.items(): t_lst.append([k, v * idf_res.get(k, 1)]) cls_word_weights.append((cls, t_lst)) return cls_word_weights 优点 实现简单，算法容易理解且解释性较强； 从IDF 的计算方法可以看出常用词（如：“我”、“是”、“的”等）在语料库中的很多文章都会出现，故IDF的值会很小；而关键词（如：“自然语言处理”、“NLP ”等）只会在某领域的文章出现，IDF 的值会比较大；故：TF-IDF 在保留文章的重要词的同时可以过滤掉一些常见的、无关紧要的词； 缺点 不能反映词的位置信息，在对关键词进行提取时，词的位置信息（如：标题、句首、句尾的词应该赋予更高的权重）； IDF 是一种试图抑制噪声的加权，本身倾向于文本中频率比较小的词，这使得IDF 的精度不高； TF-IDF 严重依赖于语料库（尤其在训练同类语料库时，往往会掩盖一些同类型的关键词；如：在进行TF-IDF 训练时，语料库中的 娱乐 新闻较多，则与 娱乐 相关的关键词的权重就会偏低 ），因此需要选取质量高的语料库进行训练； 共现矩阵 (Cocurrence matrix) 这个方法的出发点是：相似的词语会经常同时出现，并且具有相似的上下文。比如：苹果是水果。香蕉是水果。苹果和香蕉具有相似的上下文。 在深入词共现矩阵的构造和定义之前，我们先来介绍两个概念：共现和上下文窗口。 共现 — 对于给定的语料，共现指词语w1和词语w2在给定上下文窗口中共同出现的次数。 上下文窗口 — 计算共现时所指定的窗口大小，由距离和方向指定。 同样以一个例子来说明： Corpus = He is not lazy. He is intelligent. He is smart. 给定距离2和双向的上下文窗口，该例的词共现矩阵如下： 以图中的红色框和蓝色框为例。红色框表示“He”和“is”的共现次数为4，如下图所示： 而蓝色框值为0，代表词“lazy”从未出现在词“intelligent”的方圆两词之内。 在词共现矩阵中，我们可以使用行向量或者列向量来表示词语（矩阵是对称的）。但存在的问题与词频矩阵相似，向量的维度等于词典的大小V，这将导致向量是稀疏且高维的，不利于计算。在实际使用中，通常是使用PCA或SVD等技术，将词共现矩阵降维为V×k（k 优点 考虑了句子中词的顺序； 缺点 词表的长度很大，导致词的向量长度也很大； 共现矩阵也是稀疏矩阵（可以使用 SVD、PCA 等算法进行降维，但是计算量很大）； pmi计算 def cal_two_words_pmi(chars: Dict, pairs: Dict, min_pmi: float = 1e-7, pass_similar: bool = False): \"\"\" 计算pmi, pairs里的key 会尝试所有切分组合，找到最小的pmi切分 :param chars: {\"年后\":15, \"今年\":36, ...} :param pairs: {\"今年年后\":9, ...} :param min_pmi: 最小pmi阈值 :return: \"\"\" filter_ori_pairs = copy.deepcopy(pairs) log_total_chars = log(sum(chars.values())) log_total_pairs = log(sum(pairs.values())) chars = {i: log(j) - log_total_chars for i, j in chars.items()} pairs = {i: log(j) - log_total_pairs for i, j in pairs.items()} n_pairs, filter_pairs = {}, {} for word, value in pairs.items(): pair_mpi = 1e9 # 查找使pmi最小的单词 组合 cut_word_idx = 0 for idx in range(1, len(word)): if word[:idx] in chars and word[idx:] in chars: if pair_mpi > value - chars[word[:idx]] - chars[word[idx:]]: pair_mpi = value - chars[word[:idx]] - chars[word[idx:]] cut_word_idx = idx if pair_mpi > min_pmi and cut_word_idx != 0: if (not pass_similar) or \\ (pass_similar and word[:cut_word_idx] not in word[cut_word_idx:] \\ and word[cut_word_idx:] not in word[:cut_word_idx] and \\ not cal_occr(word[:cut_word_idx], word[cut_word_idx:])): n_pairs[f\"{word[:cut_word_idx]}#{word[cut_word_idx:]}\"] = pair_mpi filter_pairs[f\"{word[:cut_word_idx]}#{word[cut_word_idx:]}\"] = filter_ori_pairs[word] # filter_pairs[word] = filter_ori_pairs[word] return n_pairs, filter_pairs 语言模型 分布式表示 基于统计方法的词嵌入，这些方法都是使用很长的向量来表示一个词语，且词语的“含义”分布在高维度向量的一个或少数几个分量上（稀疏）。这些方法的主要问题在于用于表示词语的向量维度过高且非常稀疏，并且无法很好的表征词语的含义。那么一个理想的词向量应该是什么样呢？ 想象一下，我们身处于一个充满词语的空间，这个空间中，相似的词语们组成一个“家族”抱团取暖，它们的距离比较近；不相似的词语身处不同的“家族”，距离较远。如“我”“你”“他”“吴彦祖”这些词的在词空间的距离比较近，“香蕉”和“电脑”在词空间的距离比较远。那么词嵌入其实就是某种将词空间映射到向量空间的方法。 我们希望这些词语映射到一个固定长度的稠密低维度（相对于词表大小而言）向量，且依然保持以上特性，可以使用向量之间的距离来度量词语之间的相似性，这就是Distributed Representation。 基于统计的词嵌入方法产生的向量非常稀疏，词语的含义集中分布在向量的一个或少数几个非零分量上。而“Distributed”其中文意思是“分布式”，意思是指将词语的语义均匀分布到向量的各个分量上， 每个非零分量都承担其一部分含义。 语言模型就是用来计算一个句子的概率的模型，也就是判断一句话是否是人话的概率 可以将句子看做单词序列$\\omega{1}、\\omega{2}...\\omega_{n}$，那么语言模型的目标就是计算 > P(\\omega_{1},\\omega_{2}...\\omega_{n})=P(\\omega_{1})P(\\omega_{2}|\\omega_{1})...P(\\omega_{n}|\\omega_{1},\\omega_{2}...\\omega_{n-1}) > 可是这样的方法存在两个致命的缺陷： 参数空间过大：条件概率P(wn|w1,w2,..,wn-1)的可能性太多，无法估算，不可能有用； 数据稀疏严重：对于非常多词对的组合，在语料库中没有出现，依据最大似然估计得到的概率会是0。 解决方案一： 马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。 如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram 假设一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram 一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关。而这些概率参数都是可以通过大规模语料库来计算 实践中用的最多的就是bigram和trigram了，高于四元的用的非常少，由于训练它须要更庞大的语料，并且数据稀疏严重，时间复杂度高，精度却提高的不多 解决方案二：引入平滑技术。平滑技术的思想和目的是将数据集中看到的概率分配一点给未出现的数据（避免概率为0），并且保持总的概率和为1。类似于引入了正则化，对参数进行了约束。这些都是机器学习中解决模型过拟合，提高模型泛化能力的方法。 思考一个问题： 句子1：“A dog is running in the room” 句子2：\"A cat is running in the room\" 两个句子哪个概率大一些？直觉上来说应该是相似的，但统计语言模型非常受数据集的影响，无法考虑词语与词语的相似度，所以以上两个句子的概率计算结果可能是相差很大的。比如数据集中句子1出现了100次，句子2只出现了1次，那最后的计算结果P(句子1)将远大于P(句子2)。下面介绍的前向神经网络语言模型能比较好地解决这个问题。 word2vec word2vec是如何得到词向量的？ BERT发展史（三）全方位多角度理解Word2Vec 文本表示 - 泰勒不会展开 - 博客园 one-hot向量作为word2vec的输入，通过word2vec训练低维词向量（word embedding） 目前有两种训练模型（CBOW和Skip-gram），两种加速算法（Negative Sample与Hierarchical Softmax） Word2Vec包含了两种词训练模型：CBOW模型和Skip-gram模型。 CBOW：利用上下文的词预测中心词； SKIP-GRAM：利用中心词预测上下文的词； 它们的结构仅仅是输入层和输出层不同 CBOW模型和Skip-gram模型 先来看着这个结构图，用自然语言描述一下CBOW模型的流程： CBOW模型结构图 >NOTE：花括号内{}为解释内容. > >1. 输入层：上下文单词的onehot. {假设单词向量空间dim为V，上下文单词个数为C} >2. 所有onehot分别乘以共享的输入权重矩阵W. {V*N矩阵，N为自己设定的数，初始化权重矩阵W} >3. 所得的向量 {因为是onehot所以为向量} 相加求平均作为隐层向量, size为1*N. >4. 乘以输出权重矩阵W' {N*V} >5. 得到向量 {1*V} 激活函数处理得到V-dim概率分布 {PS: 因为是onehot嘛，其中的每一维斗代表着一个单词}，概率最大的index所指示的单词为预测出的中间词（target word） >6. 与true label的onehot做比较，误差越小越好 假设我们现在的Corpus是这一个简单的只有四个单词的document： {I drink coffee everyday} 我们选coffee作为中心词，window size设为2 也就是说，我们要根据单词\"I\",\"drink\"和\"everyday\"来预测一个单词，并且我们希望这个单词是coffee。 ![img](res/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B_%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/v2-3e75211b3b675f17a232f29fae0982bc_720w.jpg) ![img](res/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B_%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/v2-abd3c7d6bc76c01266e8ddd32acfe31a_720w.jpg) ![img](res/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B_%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/v2-66655880a87789eaba5dd6f5c5033e94_720w.jpg) ![img](res/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B_%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/v2-5325f4a5d1fbacefd93ccb138b706a69_720w.jpg) ![img](res/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B_%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/v2-1713450fa2a0f37c8cbcce4ffef04baa_720w.jpg) 假设我们此时得到的概率分布已经达到了设定的迭代次数，那么现在我们训练出来的look up table应该为矩阵W。即，**任何一个单词的one-hot表示乘以这个矩阵都将得到自己的word embedding。** **优点** 1. 考虑到词语的上下文，学习到了语义和语法的信息； 2. 得到的词向量维度小，节省存储和计算资源； 3. 通用性强，可以应用到各种*NLP* 任务中； **缺点** 1. 词和向量是一对一的关系，无法解决多义词的问题； 2. word2vec是一种静态的模型，虽然通用性强，但无法真的特定的任务做动态优化； ![img](res/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B_%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9E%8B/1877b44017ec4d18b8010e68ad67a239_th.png) Word2Vec效果图展示 fasttext fasttext的模型与CBOW类似，实际上，fasttext的确是由CBOW演变而来的。CBOW预测上下文的中间词，fasttext预测文本标签。与word2vec算法的衍生物相同，稠密词向量也是在训练神经网络的过程中得到的。 GloVe GloVe 是斯坦福大学Jeffrey、Richard 等提供的一种词向量表示算法，GloVe 的全称是Global Vectors for Word Representation，是一个基于全局词频统计（count-based & overall staticstics）的词表征（word representation）算法。该算法综合了global matrix factorization（全局矩阵分解） 和 local context window（局部上下文窗口） 两种方法的优点。 优点 考虑到词语的上下文、和全局语料库的信息，学习到了语义和语法的信息； 得到的词向量维度小，节省存储和计算资源； 通用性强，可以应用到各种NLP 任务中； 缺点 词和向量是一对一的关系，无法解决多义词的问题； glove也是一种静态的模型，虽然通用性强，但无法真的特定的任务做动态优化； ELMO word2vec 和 glove 算法得到的词向量都是静态词向量（静态词向量会把多义词的语义进行融合，训练结束之后不会根据上下文进行改变），静态词向量无法解决多义词的问题（如：“我今天买了7斤苹果” 和 “我今天买了苹果7” 中的 苹果 就是一个多义词）。而ELMO模型进行训练的词向量可以解决多义词的问题。 ELMO 的全称是“ Embedding from Language Models ”，这个名字不能很好的反映出该模型的特点，提出ELMO 的论文题目可以更准确的表达出该算法的特点“ Deep contextualized word representation ”。 该算法的精髓是：用语言模型训练神经网络，在使用word embedding 时，单词已经具备上下文信息，这个时候神经网络可以根据上下文信息对word embedding 进行调整，这样经过调整之后的word embedding 更能表达在这个上下文中的具体含义，这就解决了静态词向量无法表示多义词的问题。 网络模型 过程 上图中的结构使用字符级卷积神经网络（convolutional neural network, CNN）来将文本中的词转换成原始词向量（raw word vector） ； 将原始词向量输入双向语言模型中第一层 ； 前向迭代中包含了该词以及该词之前的一些词汇或语境的信息（即上文）； 后向迭代中包含了该词以及该词之后的一些词汇或语境的信息（即下文） ； 这两种迭代的信息组成了中间词向量（intermediate word vector）； 中间词向量被输入到模型的下一层 ； 最终向量就是原始词向量和两个中间词向量的加权和； 效果 如上图所示： 使用glove训练的词向量中，与 play 相近的词大多与体育相关，这是因为语料中与play相关的语料多时体育领域的有关； 在使用elmo训练的词向量中，当 play 取 演出 的意思时，与其相近的也是 演出 相近的句子； Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/特征工程_图像增强.html":{"url":"chapters/特征工程_图像增强.html","title":"特征工程_图像增强","keywords":"","body":"图像增强 [TOC] 图像增强 > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/模型评估_评估指标.html":{"url":"chapters/模型评估_评估指标.html","title":"模型评估_评估指标","keywords":"","body":"评估指标准确率(Accuracy)精确度(Precision)召回率(Recall)均方根误差(RMSE)F1得分P-R曲线ROC曲线ROC曲线概念绘制ROC曲线ROC和P-R曲线AUC曲线聚类指标回归指标 [TOC] 评估指标 不可不知的11个重要机器学习模型评估指标 准确率(Accuracy) 准确率是指分类正确的样本占总样本个数的比例，即 >Accurcy=\\frac{n_{correct}}{n_{total}}=\\frac{TP+TN}{TP+TN+FP+FN} > 其中n_{correct}为被正确分类的样本个数，n_{total}为总样本的个数。 准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。 所以，当不同类别的样本比例非常不均衡时， 占比大的类别往往成为影响准确率的最主要因素。 错误率是指分类错误的样本数占样本总数的比例，即 >ErrorRate=1-Accurcy=\\frac{FP+FN}{TP+TN+FP+FN} > 精确度(Precision) 精确率是指分类正确的正样本个数占分类器判定为正样本的样本个数的比例。 召回率是指分类正确的正样本个数占真正的正样本个数的比例。 在排序问题中，通常没有一个确定的闽值把得到的结果直接判定为正样本或负样本，而是采用TopN返回结果的Precision值和Recall值来衡量排序模型的性能，即认为模型返回的Top N的结果就是模型判定的正样本，然后计算前N个位置上的准确率Precision@N和前N个位置上的召回率Recall@N。 相关(Relevant)，正类 无关(NonRelevant)，负类 被检索到(Retrieved） true positives(TP正类判定为正类，例子中就是正确的判定“这位是女生\") false positives(FP负类判定为正类，“存伪\"，例子中就是分明是男生却判断为女生，当下伪娘横行，这个错常有人犯) 未被检索到(not Retrieved) false negatives(FN正类判定为负类，“去真”，例子中就是，分明是女生，这哥们却判断为男生--梁山伯同学犯的错就是这个） true negatives(TN负类判定为负类，也就是一个男生被判断为男生， 像我这样的纯爷们一准儿就会在此处） 根据这四种数据，有四个比较重要的比率： >True Positive Rate(真正率TPR或灵敏度sensitivity)=TPR=\\frac{TP}{TP+FN}=\\frac{正样本预测结果数}{正样本实际数} \\\\ >True Negative Rate(真负率TNR或特指度specificity)=TNR=\\frac{TN}{TN+FP}=\\frac{负样本预测结果数}{负样本实际数} \\\\ >False Positive Rate(假正率FPR)=FPR=\\frac{FP}{FP+TN}=\\frac{被预测为正的负样本结果数}{负样本实际数} \\\\ >False Negative Rate(假负率FNR)=FNR=\\frac{FN}{TP+FN}=\\frac{被预测为负的正样本结果数}{正样本实际数} \\\\ > 灵敏度表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力： >Sensitive = \\frac{TP}{P} > 特效度表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力： >specificity = \\frac{TN}{N} > 精确率(precision)计算的是所有“正确被检索的结果（TP）“占所有“实际被检索到的(TP+FP)\"的比例： >P=\\frac{TP}{TP+FP} > 在例子中就是希望知道此君得到的所有人中，正确的人(也就是女生)占有的比例，所以其precision也就是40%(\\frac{20女生}{(20女生+30误判为女生的男生)}). 召回率(Recall) 召回率(recall)的公式是R=\\frac{TP}{TP+FN}，它计算的是所有“正确被检索的结果(TP)“占所有“应该检索到的结果(TP+FN)\"的比例. 在例子中就是希望知道此君得到的女生占本班中所有女生的比例，所以其recall也就是100%(\\frac{20女生}{(20女生+0误判为男生的女生)}） 均方根误差(RMSE) 概念: RMSE经常被用来衡量回归模型的好坏，计算公式为 >RMSE=\\sqrt{\\frac{\\sum_{i=1}^{n}{(y_i-\\hat y)^2}}{n}} > 其中，y_i 是第i个样本点的真实值，\\hat y 是第i个样本点的预测值，n是样本点的个数。 ​ 一般情况下，RMSE能够很好地反映回归模型预测值与真实值的概念偏离程度。 缺点: 但在实际问题中，如果存在个别偏离程度非常大的离群点(Outlier)时，即使离群点数量非常少，也会让RMSE指标变得很差。 回到问题中来，模型在95%的时间区间内的预测误差都小于1%， 这说明在大部分时间区间内，模型的预测效果都是非常优秀的。然而RMSE却一直很差，这很可能是由于在其他的5%时间区间内存在非常严重的离群点。 事实上，在流量预估这个问题中，噪声点确实是很容易产生的，比如流量特别小的美剧、刚上映的美剧或者刚获奖的美剧，甚至一些相关社交媒体突发事件带来的流量，都可能会造成离群点。 解决: 可以从三个角度来思考。 第一， 如果我们认定这些离群点是“噪声点”的话，就需要在数据预处理的阶段把这些噪声点过滤掉。 第二，如果不认为这些离群点是“噪声点”的 话，就需要进一步提高模型的预测能力，将离群点产生的机制建模进去(这是一个宏大的话题，这里就不展开讨论了)。 第三，可以找一个更合适的指标来评估该模型。 关于评估指标，其实是存在比RMSE的鲁棒性更好的指标，比如平均绝对百分比误差(Mean Absolute Percent Error，MAPE)，它定义为 >MAPE=\\sum_{i=1}{n}{|\\frac{y_i-\\hat y}{y_i}|}\\times \\frac{100}{n} > 相比RMSE，MAPE相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。 F1得分 除此之外，F1score和ROC曲线也能综合地反映一个排序模型的性能。 F1score是精准率和召回率的调和平均值，它定义为 >F1=\\frac{2 \\times precision \\times recall}{precision + recall} > P-R曲线 为了综合评估一个排序模型的好坏，不仅要看模型在不同TopN下的Precision@N和Recall@N，而且最好绘制出模型的P-R(Precision-Recall)曲线。 P-R曲线的横轴是召回率，纵轴是精确率。 对于一个排序模型来说， 其P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。 整条P-R曲线是通过将阈值从高到低移动而生成的。 图是P-R曲线样例图，其中实线代表模型A的P-R曲线，虚线代表模型B的P-R曲线。 原点附近代表当阀值最大时模型的精确率和召回率。 ROC曲线 ROC曲线概念 ROC曲线是Receiver Operating Characteristic Curve的简称， 中文名为“受试者工作特征曲线”。 ROC曲线源于军事领域，而后在医学领域应用甚广，“受试者工作特征曲线”这一名称也正是来自医学领域。 ROC曲线的横坐标为假阳性率(False Positive Rate, FPR)；纵坐标为真阳性率(True Positive Rate, TPR)。FPR和TPR的计算方法分别为 >FPR=\\frac{FP}{N} ,TPR=\\frac{TP}{P} > 式中，P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被分类器预测为正样本的个数。 假设有10位疑似癌症患者，其中有3位很不幸确实患了癌症(P=3)，另外7位不是癌症患者(N=7)。医院对这10位疑似患者做了诊断，诊断出3位癌症患者，其中有2位确实是真正的患者(TP=2)。 那么真阳性率TPR=\\frac{TP}{P}=\\frac{2}{3}。对于7位非癌症患者来说，有一位很不幸被误诊为癌症患者(FP=1)，那么假阳性率FPR=\\frac{FP}{N}=\\frac{1}{7}。 对于“该医院”这个分类器来说，这组分类结果就对应ROC曲线上的一个点(\\frac{1}{7}, \\frac{2}{3}）。 事实上，ROC曲线是通过不断移动分类器的“截断点”来生成曲线上的一组关键点的。 通过下面的例子进一步来解释“截断点”的 概念。 在二值分类问题中，模型的输出一般都是预测样本为正例的概率。 样本序号 真实标签 模型输出概率 样本序号 真实标签 模型输出概率 1 p 0.9 11 p 0.4 2 p 0.8 12 n 0.39 3 n 0.7 13 p 0.38 4 p 0.6 14 n 0.37 5 p 0.55 15 n 0.36 6 p 0.54 16 n 0.35 7 n 0.53 17 p 0.34 8 n 0.52 18 n 0.33 9 p 0.51 19 p 0.30 10 n 0.505 20 n 0.1 假设测试集中有20个样本，表2.1是模型的输出结果。样本按照预测概率从高到低排序。在输出最终的正例、负例之前，我们需要指定一个 阈值，预测概率大于该阀值的样本会被判为正例，小于该阈值的样本则会被判为负例。 比如，指定阀值为0.9，那么只有第一个样本会被预测为正例，其他全部都是负例。 上面所说的“截断点”指的就是区分正 预测结果的阈值。 通过动态地调整截断点，从最高的得分开始(实际上是从正无穷开始，对应着ROC曲线的零点)，逐渐调整到最低得分，每一个截断点都会对应一个FPR和TPR，在ROC图上绘制出每个截断点对应的位置， 再连接所有点就得到最终的ROC曲线。 绘制ROC曲线 就本例来说，当截断点选择为正无穷时，模型把全部样本预测为负例，那么FP和TP必然都为0，FPR和TPR也都为0，因此曲线的第 一个点的坐标就是(0,0)。 当把截断点调整为0.9时，模型预测1号样本为正样本，并且该样本确实是正样本，因此TP=1，20个样本中， 所有正例数量为P=10，故TPR=\\frac{TP}{P}=\\frac{1}{10}； 这里没有预测错的正样本， 即FP=0，负样本总数N=10，故FPR=\\frac{FP}{N}=\\frac{0}{10}=0，对应ROC曲线上的点(0,0.1)。 依次调整截断点，直到画出全部的关键点，再连接关键点即得到最终的ROC曲线，如下图所示。 其实，还有一种更直观地绘制ROC曲线的方法。 首先，根据样本 标签统计出正负样本的数量，假设正样本数量为P，负样本数量为N； 接下来，把横轴的刻度间隔设置为1/N，纵轴的刻度间隔设置为1/P； 再根据模型输出的预测概率对样本进行排序（从高到低）； 依次遍历样 本，同时从零点开始绘制ROC曲线，每遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在(1,1)这个点， 整个ROC曲线绘制完成。 ROC和P-R曲线 可以看出，P-R曲线发生了明显的变化，而ROC曲线形状基本不变。 这个特点让ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。 这有什么实际意义呢？在很多实际问题中，正负样本数量往往很不均衡。比如，计算广告领域经常涉及转化率模型，正样本的数量往往是负样本数量的1/1000甚至1/10000。 若选择不同的测试集，P-R曲线的变化就会非常大，而ROC曲线则能够更加稳定地反映模型本身的好坏。 所以，ROC曲线的适用场景更多，被广泛用于排序、推荐、广告等领域。 但需要注意的是，选择P-R曲线还是ROC曲线是因实际问题而异的，如果研究者希望更多地看到模型在特定数据集上的表现，P-R曲线则能够更直观地反映其性能。 AUC曲线 顾名思义，AUC指的是ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能。 计算AUC值只需要沿着ROC横轴做积分就可以了。 由于ROC曲线一般都处于y=x这条直线的上方(如果不是的话，只要把模型预测的概率反转成1-p就可以得到一个更好的分类器)，所以AUC的取值一般在[0.5,1]之间。 AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。 聚类指标 一个好的聚类方法可以产生高品质簇，是的簇内相似度高，簇间相似度低。一般来说，评估聚类质量有两个标准，内部质量评价指标和外部评价指标。 3.1.9.1 内部质量评价标准 内部评价指标是利用数据集的属性特征来评价聚类算法的优劣。通过计算总体的相似度，簇间平均相似度或簇内平均相似度来评价聚类质量。评价聚类效果的高低通常使用聚类的有效性指标，所以目前的检验聚类的有效性指标主要是通过簇间距离和簇内距离来衡量。这类指标常用的有CH(Calinski-Harabasz)指标等 CH指标： >CH(K)=\\frac{tr(B)/(K-1)}{tr(W)/(N-K)} \\\\ >tr(B)=\\sum_{j=1}^{k}{||z_j-z||^2} \\\\ >tr(W)=\\sum_{j=1}^{k}{\\sum_{x_i \\in z_k}{||x_i-z_j||^2}} > 其中tr(B)表示类间距离差矩阵的迹，tr(W)表示类内差矩阵的迹，z是整个数据集的均值，z_j是第j个簇c_j的均值，N代表聚类的个数， K代表当前的类。值越大，CH(K)聚类效果越好，CH主要计算簇间距离与簇内距离的比值 簇的凝聚度： >SSE=\\sum_{i=1}^{r}{\\sum_{j=1}^{n_i}{(X_{ij}-\\bar X_i)^2}} \\\\ >\\bar X_i=\\frac{1}{n_i}\\sum_{j=1}^{n_i}{X_{ij}},i=1,2,...,r > 簇内点对的平均距离反映了簇的凝聚度，一般使用组内误差平方(SSE)表示 簇的邻近度： 簇的邻近度用组间平方和(SSB)表示，即簇的质心C_i到簇内所有数据点的总平均值c的距离的平方和 3.1.9.2 外部质量评价标准 外部质量评价指标是基于已知分类标签数据集进行评价的，这样可以将原有标签数据与聚类输出结果进行对比。 外部质量评价指标的理想聚类结果是：具有不同类标签的数据聚合到不同的簇中，具有相同类标签的数据聚合相同的簇中。外部质量评价准则通常使用熵，纯度等指标进行度量。 熵： 簇内包含单个类对象的一种度量。对于每一个簇，首先计算数据的类分布，即对于簇i，计算簇i的成员属于类j的概率 >p_{ij}=\\frac{m_{ij}}{m_i} > 其中m_i表示簇i中所有对象的个数，而m_{ij}是簇i中类j的对象个数。使用类分布，用标准公式： >e_i=-\\sum_{j=1}^{K}{p_{ij}log_2{p_ij}} > 计算每个簇i的熵，其中K是类个数。簇集合的总熵用每个簇的熵的加权和计算即： >e=\\sum_{i=1}^{K}{\\frac{m_i}{m}e_i} > 其中K是簇的个数，而m是簇内数据点的总和 纯度： 簇内包含单个类对象的另外一种度量。簇i的纯度为p_i=\\max_{a}{p_{ij}}，而聚类总纯度为： >purity=\\sum_{i=1}^{K}{\\frac{m_i}{m}p_i} > 08 聚类算法 - 聚类算法的衡量指标 机器学习（7）——聚类算法 回归指标 SSE(和方差、误差平方和)：The sum of squares dueto error MSE(均方差、方差)：Meansquared error RMSE(均方根、标准差)：Root mean squared error R-square(确定系数)：Coefficientof determination Adjusted R-square：Degree-of-freedomadjusted coefficient of determination SSE： >SSE = \\sum(Y_{actual} - Y_{predict})^2 > R-square： >R^2 = 1-\\frac{\\sum(Y_{actual}-Y_{predict})^2}{\\sum(Y_{actual}-Y_{mean})^2}=1-\\frac{SSE}{SST} \\\\ >SST = \\sum_{i=1}^{n}w_i(Y_{actual} - Y_{mean})^2 > Adjusted R-square： >R^2 _{adjusted}=1-\\frac{(1-R^2)(n-1)}{n-p-1} > 其中n是样本数量；p是特征数量；R^2是决定系数，该指标消除了样本数量和特征数量的影响，做到了真正的0~1，越大越好。 AIC赤池信息准则： >AIC=e^\\frac{2k}{T}\\frac{\\sum_{t=1}^{T}e_t^2}{T} \\\\ >e^\\frac{2k}{T}:惩罚因子 > 估计模型拟合数据的优良性，AIC值越小说明模型拟合得越好 AIC和BIC准则 模型选择方法：AIC和BIC Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/模型评估_AB测试.html":{"url":"chapters/模型评估_AB测试.html","title":"模型评估_AB测试","keywords":"","body":"A/B测试 [TOC] A/B测试 A/B测试是在线测试，在此之前需要离线测试： 离线评估无法完全消除模型过拟合的影响 离线评估无法完全还原线上的工程环境 线上系统的某些商业指标在离线评估中无法计算 A/B测试主要方法:用户分桶(即将用户分为实验组和对照组。样本独立性和采样的无偏性，确保每个用户每次只能分到同一个桶中) 需要进行在线A/B测试的原因如下。 离线评估无法完全消除模型过拟合的影响，因此，得出的离 线评估结果无法完全替代线上评估结果。 离线评估无法完全还原线上的工程环境。一般来讲，离线评 估往往不会考虑线上环境的延迟、数据丢失、标签数据缺失等情况。因 此，离线评估的结果是理想工程环境下的结果。 线上系统的某些商业指标在离线评估中无法计算。离线评估 一般是针对模型本身进行评估，而与模型相关的其他指标，特别是商业 指标，往往无法直接获得。比如，上线了新的推荐算法，离线评估往往 关注的是ROC曲线、P-R曲线等的改进，而线上评估可以全面了解该 推荐算法带来的用户点击率、留存时长、PV访问量等的变化。这些都 要由A/B测试来进行全面的评估。 进行A/B测试的主要手段是进行用户分桶，即将用户分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型。 在分桶的过程中，要注意样本的独立性和采样方式的无偏性，确保同一 个用户每次只能分到同一个桶中，在分桶过程中所选取的$user_id$需要 是一个随机数，这样才能保证桶中的样本是无偏的。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/模型评估_过拟合和欠拟合.html":{"url":"chapters/模型评估_过拟合和欠拟合.html","title":"模型评估_过拟合和欠拟合","keywords":"","body":"过拟合和欠拟合降低过拟合降低欠拟合 [TOC] 过拟合和欠拟合 过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。 欠拟合指的是模型在训练和预测时表现都不好的情况。下图形象地描述了过拟合和欠拟合的区别。 降低过拟合 数据入手，获得更多的训练数据。使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减小噪声的影响。当然，直接增加实验数据一般是很困难的，但是可以通过一定的规则来扩充训练数据。比如， 在图像分类的问题上，可以通过图像的平移、旋转、缩放等方式扩 充数据；更进一步地，可以使用生成式对抗网络来合成大量的新训练数据。 降低模型复杂度。在数据较少时，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。 例如，在神经网络模型中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。 正则化方法。给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。以L2正则化为例： $$ > C=C_0+\\frac{\\lambda}{2n}.\\sum_{i}{w_i^2} > > 这样，在优化原来的目标函数C_0$$的同时，也能避免权值过大带来的过拟合风险。 集成学习方法。集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法。 降低欠拟合 添加新特征。当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。通过挖掘“上下文特征”“ID类特征”“组合特征”等新的特征，往往能够取得更好的效果。在深度学习潮流中， 有很多模型可以帮助完成特征工程，如因子分解机、梯度提升决策树、 Deep-crossing等都可以成为丰富特征的方法。 增加模型复杂度。简单模型的学习能力较差，通过增加模型的复杂度可以使模型拥有更强的拟合能力。例如，在线性模型中添加高欠项，在神经网络模型中增加网络层数或神经元个数等。 减小正则化系数。正则化是用来防止过拟合的，但当模型出现欠拟合现象时，则需要有针对性地减小正则化系数。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/模型评估_超参数选择.html":{"url":"chapters/模型评估_超参数选择.html","title":"模型评估_超参数选择","keywords":"","body":"超参数选择网格搜索随机搜索贝叶斯优化 [TOC] 超参数选择 为了进行超参数调优，我们一般会采用网格搜索、随机搜索、贝叶斯 优化等算法。在具体介绍算法之前，需要明确超参数搜索算法一般包括哪几个要素。 目标函数，即算法需要最大化/最小化的目标 搜索范围， 一般通过上限和下限来确定 算法的其他参数，如搜索步长。 网格搜索 网格搜索可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范围内的所有的点来确定最优值。 如果采用较大的搜索范围以及较小的步长，网格搜索有很大概率找到全局最优值。 然而，这种搜索方案十分消耗计算资源和时间，特别是需要调优的超参数比较多的时候。 因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。这种操作方案可以降低所需的时间和计算 量，但由于目标函数一般是非凸的，所以很可能会错过全局最优值。 随机搜索 随机搜索的思想与网格搜索比较相似，只是不再测试上界和下界之间的所有值，而是在搜索范围中随机选取样本点。 它的理论依据是，如果样本点集足够大，那么通过随机采样也能大概率地找到全局最优值， 或其近似值。随机搜索一般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。 贝叶斯优化 贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全不同的方法。 网格搜索和随机搜索在测试一个新点时，会忽略前一 个点的信息；而贝叶斯优化算法则充分利用了之前的信息。 贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。 具体来说，它学习目标函数形状的方法是，首先根据先验分布，假设一个搜集函数；然后，每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；最后，算法测试由后验分布给出的全 局最值最可能出现的位置的点。 对于贝叶斯优化算法，有一个需要注意的地方，一旦找到了一个局部最优值，它会在该区域不断采样，所以很容易陷入局部最优值。 为了弥补这个缺陷，贝叶斯优化算法会在探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利 用”则是根据后验分布在最可能出现全局最值的区域进行采样。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/模型评估_模型评估方法.html":{"url":"chapters/模型评估_模型评估方法.html","title":"模型评估_模型评估方法","keywords":"","body":"模型评估方法Holdout检验交叉验证自助法 [TOC] 模型评估方法 Holdout检验 Holdout检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分成训练集和验证集两部分。 比方说，对于一个点击率预测模 型，我们把样本按照70%~30%的比例分成两部分，70%的样本用于模型训练；30%的样本用于模型验证，包括绘制ROC曲线、计算精确率和召回率等指标来评估模型性能。 Holdout 检验的缺点很明显，即在验证集上计算出来的最后评估指标与原始分组有很大关系。 为了消除随机性，研究者们引入了“交叉检验”的思想。 交叉验证 k-fold交叉验证: 首先将全部样本划分成k个大小相等的样本子集； 依次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估； 最后把k次评估指标的平均值作为最 终的评估指标。在实际实验中，k经常取10。 留一验证: 每次留下1个样本作为验证集，其余所有样本作为测试集。 样本总数为n，依次对n个样本进行遍历，进行n次验证，再将评估指标求平均值得到最终的评估指标。 在样本总数较多的情况下，留一验证法的时间开销极大。 事实上，留一验证是留p验证的特例。留p验证是每次留下p个样本作为验证集，而从n个元素中选择p个元素有C种可能，因此它的时间开销更是远远高于留一验证，故而很少在实际工程中被应用。 自助法 不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。 然而，当样本规模比较小时，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。 有没有能维诗训练集样本规模的验证方法呢？自助法可以比较好地解决这个问题。 自助法是基于自助采样法的检验方法。 对于总数为n的样本集合， 进行n次有放回的随机抽样，得到大小为n的训练集。 n次采样过程中， 有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/降维_PCA主成分分析.html":{"url":"chapters/降维_PCA主成分分析.html","title":"降维_PCA主成分分析","keywords":"","body":"主成分分析(PCA) 主成分分析(PCA) > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/降维_LDA线性判别分析.html":{"url":"chapters/降维_LDA线性判别分析.html","title":"降维_LDA线性判别分析","keywords":"","body":"线性判别分析(LDA) [TOC] 线性判别分析(LDA) > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/监督学习_朴素贝叶斯分类.html":{"url":"chapters/监督学习_朴素贝叶斯分类.html","title":"监督学习_朴素贝叶斯分类","keywords":"","body":"朴素贝叶斯分类(Naive Bayesian)贝叶斯定理全概率公式流程步骤例子朴素贝叶斯的概率模型 [TOC] 朴素贝叶斯分类(Naive Bayesian) 贝叶斯定理是关于随机事件A和B的条件概率的一则定理。 通常，事件A在事件B(发生)的条件下的概率，与事件B在事件A(发生)的条件下的概率是不一样的，但它们两者之间是有确定的关系的，贝叶斯定理陈述了这个关系。 贝叶斯定理的一个主要应用为贝叶斯推理，它是一种建立在主观判断基础之上的推理方法，也就是说，你只需要先预估一个值，然后再去根据实际结果去不断修正，不需要任何客观因素。 这种推理方式需要大量的计算，因此一直遭到其他人的诟病，无法得到广泛的应用，直到计算机的高速发展，并且人们发现很多事情都是无法事先进行客观判断的，因此贝叶斯推理才得以东山再起。 新手入门：带你搞懂朴素贝叶斯分类算法 贝叶斯定理 >P(A|B)=\\frac{P(B|A)P(A)}{P(B)} > P(A|B): 在B条件下的事件A的概率，在贝叶斯定理中，条件概率也被称为后验概率，即在事件B发生之后，我们对事件A概率的重新评估。 P(B|A): 在A条件下的事件B的概率，与上一条同理。 P(A)与P(B)被称为先验概率(也被称为边缘概率)，即在事件B发生之前，对事件A概率的一个推断(不考虑任何事件B方面的因素)，后面同理。 \\frac{P(B|A)}{P(B)}被称为标准相似度，它是一个调整因子，主要是为了保证预测概率更接近真实概率。 根据这些术语，贝叶斯定理表述为：后验概率=标准相似度*先验概率。 全概率公式 全概率公式是将边缘概率与条件概率关联起来的基本规则，它表示了一个结果的总概率 可以通过几个不同的事件来实现 全概率公式将对一复杂事件的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题 >P(B)=\\sum_i=1{}^{n}{P(A_{i})P(B|A_i)} > 假定一个样本空间S，它是两个事件A与C之和，同时事件B与它们两个都有交集，如下图所示： 那么事件B的概率可以表示为P(B)=P(B \\cap A)+P(B \\cap C) 通过条件概率，可以推断出P(B \\cap A)=P(B|A)P(A)，所以P(B)=P(B|A)P(A)+P(B|C)P(C) 这就是全概率公式，即事件B的概率等于事件A与事件C的概率分别乘以B对这两个事件的条件概率之和。 流程步骤 输入：训练数据T=\\{(x_1,y_1 ),(x_2,y_2 ),...,(x_N,y_N )\\}，其中x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(N)})^T，x_i^{(j)}是第i个样本的第j个特征，x_i^{(j)} \\in \\{ a_{j1},a_{j2},...,a_{jS_j} \\}，a_{jl}是第j个特征可能取的第l个值，j＝1,2,...,n，l＝1,2,...,S_j ，y_i \\in \\{c_1,c_2,...c_K \\}；实例x； 输出：实例x的分类 计算先验概率及条件概率 >P(Y=c_k)=\\frac{\\sum_{i=1}^{N}{I(y_i=c_k)}}{N},k=1,2,...,K \\\\ >P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}{I(x_i^{(j)}=a_{jl},y_i=c_k)}}{\\sum_{i=1}^{N}{I(y_i=c_k)}} \\\\ >j=1,2,...n;l=1,2,...,S_j;k=1,2,...,K > 对于给定的实例x＝(x^{(1)},x^{(2)},...,x^{(n)})^T，计算 >P(Y=c_k)\\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_k}),k=1,2,...,K > 确定实例x的类 >y=\\arg\\max_{c_k}P(Y=c_k\\prod_{j=1}^{n}{P(X^{(j)}=x^{(j)}|Y=c_k)} > 例子 Example1: 假设有两家工厂生产并对外提供电灯泡 工厂X生产的电灯泡在99%的情况下能够工作超过5000 小时， 工厂Y生产的电灯泡在95%的情况下能够工作超过5000小时。 工厂X在市场的占有率为60%，工厂Y为40%， 如何推测出购买的灯泡的工作时间超过5000小时的概率是多少呢？ 运用全概率公式，可以得出： >Pr(A)=Pr(A|B_x).Pr(B_x)+Pr(A|B_y).Pr(B_y)=\\frac{99}{100}.\\frac{6}{10}+\\frac{95}{100}.\\frac{4}{10}=\\frac{974}{1000} > Pr(B_x)=\\frac{6}{10}: 购买到工厂X制造的电灯泡的概率。 Pr(B_y)=\\frac{4}{10}: 购买到工厂y制造的电灯泡的概率。 Pr(A|B_x)=\\frac{99}{100}: 工厂x制造的电灯泡工作时间超过5000小时的概率。 Pr(A|B_y)=\\frac{95}{100} : 工厂y制造的电灯泡工作时间超过5000小时的概率。 因此，可以得知购买一个工作时间超过5000小时的电灯泡的概率为97.4%。 Example2: 试由表的训练数据学习一个朴素贝叶斯分类器并确定x＝(2,S)^T的类标记y。 表中X^{(1)} ，X^{(2)}为特征，取值集合分别为A_1＝\\{1,2,3\\} ，A_2＝\\{S,M,L\\}，Y为类标记，Y \\in C =\\{1,-1\\}。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 x^{(1)} 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 x^{(2)} S M M S S S M M L L L M M L L Y -1 -1 1 1 -1 -1 -1 1 1 1 1 1 1 1 -1 解 　根据算法，由表容易计算下列概率： >P(Y=1)=\\frac{10}{17},P(Y=-1)=\\frac{7}{17} \\\\ >P(X^{(1)}=1|Y=1)=\\frac{3}{12},P(X^{(1)}=2|Y=1)=\\frac{4}{12},P(X^{(1)}=3|Y=1)=\\frac{5}{12} \\\\ >P(X^{(2)}=S|Y=1)=\\frac{2}{12},P(X^{(2)}=M|Y=1)=\\frac{5}{12},P(X^{(2)}=L|Y=1)=\\frac{5}{12} \\\\ >P(X^{(1)}=1|Y=-1)=\\frac{4}{9},P(X^{(1)}=2|Y=-1)=\\frac{3}{9},P(X^{(1)}=3|Y=-1)=\\frac{2}{9} \\\\ >P(X^{(2)}=S|Y=-1)=\\frac{4}{9},P(X^{(2)}=M|Y=-1)=\\frac{3}{9},P(X^{(2)}=L|Y=-1)=\\frac{2}{9} > 对于给定的x＝(2,S)^T计算： >P(Y=1)P(X^{(1)}=2|Y=1)P(X^{(2)}=S|Y=1)=\\frac{9}{15}.\\frac{3}{9}.\\frac{1}{9}=\\frac{1}{45} \\\\ >P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1)=\\frac{6}{15}.\\frac{2}{6}.\\frac{3}{6}=\\frac{1}{15} > 因为P(Y＝-1)P(X^{(1)}＝2|Y＝-1)P(X^{(2)}＝S|Y＝-1)最大，所以y＝-1。 朴素贝叶斯的概率模型 我们设一个待分类项X=f_1,f_2,...,f_n，其中每个f为x的一个特征属性，然后设一个类别集合C_1, C_2,...,C_m 然后需要计算P(C_1|X),P(C_2|X),...,P(C_m|X)，我们可以根据一个训练样本集合(已知分类的待分类项集合)，然后统计得到在各类别下各个特征属性的条件概率： >P(f_1|C_1),P(f_1|C_1),...P(f_n|C_1),...,P(f_1|C_2),P(f_2|C_2), \\\\ >...P(f_n|C_2,...,P(f_1|C_m),P(f_2|C_m),...,P(f_n|C_m), > 如果P(C_k|X)=MAX(P(C_1|X),P(C_2|X),...,P(C_mlX))，则X \\in C_k(贝叶斯分类其实就是取概率最大的那一个)。 朴素贝叶斯会假设每个特征都是独立的，根据贝叶斯定理可推得： >P（C_i|X)=\\frac{P(X|C_i)P(C_i)}{P(X)} > 由于分母对于所有类别为常数，因 此只需要将分子最大化即可，又因为各特征是互相独立的，所以最终推得： >P(X|C_i)P(C_i)=P(f_1|C_i)P(f_2|C_i),...,P(f_n|C_i)P(C_i)=P(C_i)\\prod_{j=1}^{n}{P(f_j|C_i)} > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/监督学习_决策树.html":{"url":"chapters/监督学习_决策树.html","title":"监督学习_决策树","keywords":"","body":"决策树(Decision Tree)特征选择ID3—最大信息增益C4.5—最大信息增益比CART—最大基尼指数(Gini)构造准则比较剪枝操作预剪枝后剪枝CART剪枝 [TOC] 决策树(Decision Tree) 决策树是一种自上而下，对样本数据进行树形分类的过程，由结点和有向边组成。结点分为内部结点和叶结点，其中每个内部结点表示一个特征或属性，叶结点表示类别。从顶部根结点开始，所有样本聚在一起。经过根结点的划分，样本被分到不同的子结点中。再根据子结点的特征进一步划分，直至所有样本都被归到某一个类别(即叶结点)中。 决策树作为最基础、最常见的有监督学习模型，常被用于分类问题和回归问题，在市场营销和生物医药等领域尤其受欢迎，主要因为树形结构与销售、诊断等场景下的决策过程十分相似。将决策树应用集成学习的思想可以得到随机森林、梯度提升决策树等模型。 决策树的生成包含了特征选择、树的构造、树的剪枝三个过程 特征选择 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。 如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，称这个特征是没有分类能力的。 经验上扔掉这样的特征对决策树学习的精度影响不大。 通常特征选择的准则是信息增益或信息增益比。 信息增益 在信息论与概率统计中，熵(entropy)是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为 >>P(X=x_i)=p_i,i=1,2,...,n >> 则随机变量X的熵定义为 >>H(X)=-\\sum_{i=1}^{n}{p_i \\log (p_i)} >> 在式中，若p_i＝0，则定义0log0＝0。 通常，式中的对数以2为底或以e为底(自然对数)，这时熵的单位分别称作比特(bit)或纳特(nat)。 由定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作H(p)，即 >>H(p)=-\\sum_{i=1}^{n}{p_i \\log {(p_i)}} >> 熵越大，随机变量的不确定性就越大。从定义可验证 >>0 \\leq H(p) \\leq log(n) >> 当随机变量只取两个值，例如1，0时，即X的分布为 >>P(X=1)=p,P(X=0)=1-p,0 \\leq p \\leq 1 >> 熵为 >>H(p)=-p \\log_{2}{p}-(1-p) \\log_2(1-p) >> 这时，熵H(p)随概率p变化的曲线如图所示(单位为比特)。 当p＝0或p＝1时H(p)＝0，随机变量完全没有不确定性。 当p＝0.5时，H(p)＝1，熵取值最大，随机变量不确定性最大。 设有随机变量(X,Y)，其联合概率分布为 >>P(X=x_i,Y=y_i)=p_{ij},i=1,2,...,n;j=1,2,...,n >> 条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。 随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望 >>H(Y|X)=\\sum_{i=1}^{n}{p_iH(Y|X=x_i)} >> 这里，p_i＝P(X＝x_i),i＝1,2,...,n。 当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵(empirical entropy)和经验条件熵(empirical conditional entropy)。 此时，如果有0概率，令0log0＝0。 信息增益(information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。 特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即 >>g(D,A)=H(D)-H(D|A) >> 一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息(mutual information)。 决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。 根据信息增益准则的特征选择方法是：对训练数据集(或子集)D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。 信息增益比 信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。 使用信息增益比(information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。 信息增益比: 特征A对训练数据集D的信息增益比g_R(D,A)定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比： >>g_R(D,A)=\\frac{g(D,A)}{H(D)} >> ID3—最大信息增益 对于样本集合D，类别数为K，数据集D的经验熵表示为 >H(D)=-\\sum_{k=1}^{K}{\\frac{|C_k|}{|D|} log_2{\\frac{|C_k|}{|D|}}} > 其中C_k是样本集合D中属于第k类的样本子集，|C_k|为该子集的元素个数，|D|为样本集合的元素个数。 然后计算某个特征A对于数据集D的经验条件熵为H(D|A) >H(D|A)=\\sum_{i=1}^{n}{\\frac{|D_i|}{|D|}H(D_i)}=\\sum_{i=1}^{n}{\\frac{|D_i|}{|D|}(-\\sum_{k=1}^{K}{\\frac{|D_{ik}|}{|D_i|}log_2{\\frac{|D_{ik}|}{|D_i|}}})} > 其中，D_i表示D中特征A取第i个值的样本子集，D_{ik}表示D_i中属于第k类的样本子集。 于是信息增益g(D,A)可以表示为二者之差，可得 >g(D,A)=H(D)-H(D|A) > 计数 年龄 收入 学生 信誉 是否购买 64 青 高 否 良 不买 64 青 高 否 优 不买 128 中 高 否 良 买 60 老 中 否 良 买 64 老 低 是 良 买 64 老 低 是 优 不买 64 中 低 是 优 买 128 青 中 否 良 不买 64 青 低 是 良 买 132 老 中 是 良 买 64 青 中 是 优 买 32 中 中 否 优 买 32 中 高 是 良 买 64 老 中 否 优 不买 有了上面的这些概念，我们就可以手工实现以下ID3算法的决策树生成过程。 (1) 计算对给定样本分类所需的信息熵。 如表所示，类别标签S被分两类：买或不买。其中S_1(买)=640；S_2(不买)=384。 那么总S=S_1+S_2=1024。 ​ S_1的概率p_1=640/1024=0.625；S_2的概率P_2=384/1024=0.375。 ​ >I(S_1,S_2)=I(640,384)=-p_1log(p_1)-p_2log(p_2)\\\\ >=-(p_1log(p_1)+p_2log(p_2))=0.9544 > (2) 计算每个特征的信息熵 ​ ① 先计算“年龄”特征的熵。 ​ 年龄共分三组：青年(0)、中年(1)、老年(2)。 ​ 其中青年占总样本的概率为：p(0)=384/1024=0.375； ​ 青年中买/不买比例为：128/256， S_1(买)=128，p_1=128/384；S_2(不买)=256，p_2=256/384； S=S_1+S_2=384。 ​ 根据公式： >I(S_1,S_2)=I(128,256)=0.9183 > ​ 其中中年占总样本的概率为：p(1)=256/1024=0.25；中年买/不买比例为：256/0，S_1(买）=256， p_1=1；S_2(不买)=0，p_2=0；S=S_1+S_2=256。 ​ 根据公式： >I(S_1,S_2)=I(256,0)=0 > ​ 其中老年占总样本的概率为：P(2)=384/1024=0.375；老年买/不买比例为：257/127， ​ S_1(买)=257，p_1=257/384；S_2(不买)=127,p_2=127/384；S=S_1+S_2=384。 ​ 根据公式3.3： >I(S_1,S_2)=I(257,127)=0.9157 > ​ 那么，年龄的平均信息期望： >E(年龄)=0.375 \\times 0.9183+0.25 \\times 0+0.375 \\times 0.9157=0.6877\\\\ >G(年龄)=0.9544-0.6877=0.2667 > ​ ② 计算“学生”特征的熵。 ​ 学生共分两组：是(0)、否(1)。 ​ 学生的平均信息期望： >E(学生)=0.7811 \\\\ >G(学生)=0.9544-0.7811=0 >1733 > ③ 计算“收入”特征的熵。 ​ 收入共分三组：高(0)、中(1)、低(2)。 ​ 收入的平均信息期望： >E(收入)=0.9361 \\\\ >G(收入)=0.9544-0.9361=0.0183 > ​ ④ 计算“信誉”特征的熵。 ​ 信誉共分两组：优(0)、良(1)。 ​ 信誉的平均信息期望： >E(信誉)=0.9048 \\\\ >G(信誉)=0.9544-0.9048=0.0496 > (3) 从所有的特征列中选出信息增益最大的那个作为根节点或内部节点—划分节点，划分整列，首次递归选择年龄列(G=0.2667)来划分。 (4) 根据划分节点的不同取值来拆分数据集为若干个子集，然后删去当前的特征列， 再计算剩余特征列的信息熵。如果有信息增益，就重复第二步直至划分结束。 首次划分 后，青年和老年内含有多个标签，所以可以继续划分；中年选项就只剩一个标签，就作为叶子节点。 (5) 划分结束的标志为：子集中只有一个类别标签，停止划分。 按照这样的逻辑产生的决策树结果如图所示 ​ 从图中可以看到，使用信息熵生成的决策树要比自己计算的决策树层数少。 如果数据集的特征很多，那么使用信息熵创建决策树在结构上要明显优于其他方法，可形成最优的决策树结构。 ​ ID3算法是比较早的机器学习算法，在1979年Quinlan就提出了该算法的思想。它以信息熵为度量标准，划分出决策树特征节点，每次优先选取信息量最多的属性，也就是使信息熵变为最小的属性，以构造一棵信息熵下降最快的决策树。 缺点 ID3算法的节点划分度量标准采用的是信息增益，信息增益偏向于选择特征值个数较多的特征。 而取值个数较多的特征并不一定是最优的特征，所以需要改进选择属性的节点划分度量标准。 ID3算法递归过程中需要依次计算每个特征值的，对于大型数据会生成比较复杂的决策树：层次和分支都很多，而其中某些分支的特征值概率很小，如果不加忽略就会造成过拟合的问题。即决策树对样本数据的分类精度较高，但在测试集上，分类的结果受决策树分支的影响很大。 C4.5—最大信息增益比 特征A对于数据集D的信息增益比定义为 >g_R(D,A)=\\frac{g(D,A)}{H_A(D)} > 其中 >H_A(D)=-\\sum_{i=1}^{n}{\\frac{|D_i|}{D}log_2{\\frac{D_i}{D}}} > 称为数据集D关于A的取值熵。 年龄 长相 工资 写代码 类别 小A 老 帅 高 不会 不见 小B 年轻 一般 中等 会 见 小C 年轻 丑 高 不会 不见 小D 年轻 一般 高 会 见 小E 年轻 一般 高 不会 不见 针对上述问题，我们可以求出数据集关于每个特征的取值熵为 >H_{年龄}=-\\frac{1}{5}log_2{\\frac{1}{5}}-\\frac{4}{5}log_2{\\frac{4}{5}}=0.722 \\\\ >H_{长相}=-\\frac{1}{5}log_2{\\frac{1}{5}}-\\frac{3}{5}log_2{\\frac{3}{5}}-\\frac{1}{5}log_2{\\frac{1}{5}}=1.371 \\\\ >H_{工资}=-\\frac{3}{5}log_2{\\frac{3}{5}}-\\frac{1}{5}log_2{\\frac{1}{5}}-\\frac{1}{5}log_2{\\frac{1}{5}}=1.371 \\\\ >H_{写代码}=-\\frac{3}{5}log_2{\\frac{3}{5}}-\\frac{2}{5}log_2{\\frac{2}{5}}=0.971 > 于是可计算出各个特征的信息增益比为 >g_R(D,年龄)=0.236 \\\\ >g_R(D,长相)=0.402 \\\\ >g_R(D,工资)=0.402 \\\\ >g_R(D,写代码)=1 > 信息增益比最大的是特征“写代码”，但通过信息增益比，特征“年龄”对应的指标上升了，而特征“长相”和特征“工资”却有所下降。 缺点：信息增益比偏向取值较少的特征 原因： 当特征取值较少时HA(D)的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。 使用信息增益比：基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。 CART—最大基尼指数(Gini) Gini描述的是数据的纯度，与信息熵含义类似。 >Gini(D)=1-\\sum_{k=1}^{n}{(\\frac{|C_k|}{|D|})^2} > CART在每一次迭代中选择基尼指数最小的特征及其对应的切分点进行分类。但与ID3、C4.5不同的是，CART是一颗二叉树，采用二元切割法，每一步将数据按特征A的取值切成两份，分别进入左右子树。特征A的Gini指数定义为 >Gini(D|A)=\\sum_{i=1}^{n}{\\frac{|D_i|}{|D|}}Gini(D_i) > 还是考虑上述的例子，应用CART分类准则，根据式（3.24）可计算出各个特征的Gini指数为 >Gini(D|年龄=老)=0.4, Gini(D|年龄=年轻)=0.4, \\\\ >Gini(D|长相=帅)=0.4, Gini(D|长相=丑)=0.4, \\\\ >Gini(D|写代码=会)=0, Gini(D|写代码=不会)=0, \\\\ >Gini(D|工资=高)=0.47, Gini(D|工资=中等)=0.3, \\\\ >Gini(D|工资=低)=0.4 > 在“年龄”“长相”“工资”“写代码”四个特征中，我们可以很快地发现特征“写代码”的Gini指数最小为0，因此选择特征“写代码”作为最优特征，“写代码=会”为最优切分点。按照这种切分，从根结点会直接产生两个叶结点，基尼指数降为0，完成决策树生长。 构造准则比较 通过对比三种决策树的构造准则，以及在同一例子上的不同表现，我们不难总结三者之间的差异。 首先，ID3是采用信息增益作为评价标准，除了“会写代码”这一逆天特征外，会倾向于取值较多的特征。因为，信息增益反映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性更高，也就是条件熵越小，信息增益越大。这在实际应用中是一个缺陷。比如，我们引入特征“DNA”，每个人的DNA都不同，如果ID3按照“DNA”特征进行划分一定是最优的（条件熵为0），但这种分类的泛化能力是非常弱的。因此，C4.5实际上是对ID3进行优化，通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，避免ID3出现过拟合的特性，提升决策树的泛化能力。 其次，从样本类型的角度，ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。C4.5处理连续型变量时，通过对数据排序之后找到类别不同的分割线作为切分点，根据切分点把连续属性转换为布尔型，从而将连续型变量转换多个取值区间的离散型变量。而对于CART，由于其构建时每次都会对特征进行二值划分，因此可以很好地适用于连续性变量。 从应用角度，ID3和C4.5只能用于分类任务，而CART（Classification and Regression Tree，分类回归树）从名字就可以看出其不仅可以用于分类，也可以应用于回归任务（回归树使用最小平方误差准则）。 此外，从实现细节、优化过程等角度，这三种决策树还有一些不同。比如，ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理；ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，因此最后会形成一颗二叉树，且每个特征可以被重复使用；ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的树结构进行对比。 至此，我们从构造、应用、实现等角度对比了ID3、C4.5、CART这三种经典的决策树模型。这些区别与联系总结起来容易，但在实际应用中还需要读者慢慢体会，针对不同场景灵活变通。 剪枝操作 决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。 在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning）。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型，提升模型的泛化能力。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)或代价函数(cost function)来实现。 设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有N_t 个样本点，其中k类的样本点有N_{tk}个，k＝1,2,...,K，H_t(T)为叶结点t上的经验熵，a \\geq 0为参数，则决策树学习的损失函数可以定义为 >C_a(T)=\\sum_{t=1}^{|T|}{N_tH_t(T)+\\alpha|T|} > 其中经验熵为 >H_t(T)=-\\sum_{k}{\\frac{N_{tk}}{N_t}log{\\frac{N_{tk}}{N_t}}} > 在损失函数中，将式右端的第1项记作 >C(T)=\\sum_{t=1}^{|T|}{N_tH_t(T)}=-\\sum_{t=1}^{|T|}{\\sum_{k=1}^{K}{ N_{tk}log{\\frac{N_{tk}}{N_t}} }} > 这时有 >C_{\\alpha}(T)=C(T)+\\alpha|T| > 式中，C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数 \\alpha \\geq 0控制两者之间的影响。 较大的 \\alpha促使选择较简单的模型(树)， 较小的 \\alpha促使选择较复杂的模型(树)。 \\alpha=0意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 剪枝，就是当 \\alpha确定时，选择损失函数最小的模型，即损失函数最小的子树。 当 \\alpha值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡。 可以看出，决策树生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合。 而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。 决策树生成学习局部的模型，而决策树剪枝学习整体的模型。 定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择 决策树的剪枝通常有两种方法，预剪枝(Pre-Pruning)和后剪枝(Post-Pruning)。 预剪枝 预剪枝的核心思想是在树中结点进行扩展之前，先计算当前的划分是否能带来模型泛化能力的提升，如果不能，则不再继续生长子树。此时可能存在不同类别的样本同时存于结点中，按照多数投票的原则判断该结点所属类别。预剪枝对于何时停止决策树的生长有以下几种方法。 当树到达一定深度的时候，停止树的生长。 当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。 计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继续扩展。 预剪枝具有思想直接、算法简单、效率高等特点，适合解决大规模问题。但如何准确地估计何时停止树的生长（即上述方法中的深度或阈值），针对不同问题会有很大差别，需要一定经验判断。 且预剪枝存在一定局限性，有欠拟合的风险，虽然当前的划分会导致测试集准确率降低，但在之后的划分中，准确率可能会有显著上升。 后剪枝 后剪枝的核心思想是让算法生成一棵完全生长的决策树，然后从最底层向上计算是否剪枝。 剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样按照多数投票的原则进行判断。同样地，后剪枝也可以通过在测试集上的准确率进行判断，如果剪枝过后准确率有所提升，则进行剪枝。 相比于预剪枝，后剪枝方法通常可以得到泛化能力更强的决策树，但时间开销会更大。 常见的后剪枝方法包括错误率降低剪枝(Reduced Error Pruning，REP)、悲观剪枝(Pessimistic Error Pruning，PEP)、代价复杂度剪枝(Cost Complexity Pruning，CCP)、最小误差剪枝(Minimum Error Pruning，MEP)、CVP(Critical Value Pruning)、OPP(Optimal Pruning)等方法，这些剪枝方法各有利弊，关注不同的优化角度。 CART剪枝 CART的代价复杂剪枝CCP 代价复杂剪枝主要包含以下两个步骤。 从完整决策树T_0开始，生成一个子树序列\\{T_0,T_1,*,...,T_n\\}，其中T_{i+1}由T_i生成，T_n为根节点。 在子树序列中，根据真实误差选择最佳的决策树。 步骤1从T_0开始，裁剪中T_i关于训练数据集合误差增加最小的分支以得到T_{i+1}。 具体地，当一棵树T在结点t处剪枝时，它的误差增加可以用R(t)−R(T_t)表示，其中R(t)表示进行剪枝之后的该结点误差，R(T_t)表示未进行剪枝时子树T_t的误差。 考虑到树的复杂性因素，我们用|L(T_t)|表示子树T_t的叶子结点个数，则树在结点t处剪枝后的误差增加率为 >\\alpha = \\frac{R(t)-R(T_t)}{|L(T_t)|-1} > 在得到T_i后，我们每步选择 \\alpha最小的结点进行相应剪枝。 用一个例子简单地介绍生成子树序列的方法。 假设把场景中的问题进行一定扩展，女孩需要对80个人进行见或不见的分类。 假设根据某种规则，已经得到了一棵CART决策树T_0，如图所示 此时共5个内部结点可供考虑，其中 >a(t_0)=\\frac{25-5}{6-1}=4, \\\\ >a(t_1)=\\frac{10-(1+2+0+0)}{4-1}=2.33, \\\\ >a(t_2)=\\frac{5-(1+1)}{2-1}=3, \\\\ >a(t_3)=\\frac{4-(1+2)}{2-1}=1, \\\\ >a(t_4)=\\frac{4-0}{2-1}=4 > 可见\\alpha (t_3)最小，因此对进t_3进行剪枝，得到新的子树T_1，如图所示。 而后继续计算所有结点对应的误差增加率，分别为\\alpha(t_1)=3，\\alpha(t_2)=3，\\alpha(t_4)=4。因此对t_1进行剪枝，得到T_2，如图所示。 此时\\alpha (t_0)=6.5，\\alpha (t_2)=3，选择t_2进行剪枝，得到T_3。 于是只剩下一个内部结点，即根结点，得到T_4。 在步骤2中，我们需要从子树序列中选出真实误差最小的决策树。 CCP给出了两种常用的方法： 一种是基于独立剪枝数据集，该方法与REP类似，但由于其只能从子树序列\\{ T_0,T_1,...,T_n \\}中选择最佳决策树，而非像REP能在所有可能的子树中寻找最优解，因此性能上会有一定不足。 另一种是基于k折交叉验证，将数据集分成k份，前k−1份用于生成决策树，最后一份用于选择最优的剪枝树。重复进行N次，再从这N个子树中选择最优的子树。 代价复杂度剪枝使用交叉验证策略时，不需要测试数据集，精度与REP差不多，但形成的树复杂度小。 而从算法复杂度角度，由于生成子树序列的时间复杂度与原始决策树的非叶结点个数呈二次关系，导致算法相比REP、PEP、MEP等线性复杂度的后剪枝方法，运行时间开销更大。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/监督学习_K近邻法.html":{"url":"chapters/监督学习_K近邻法.html","title":"监督学习_K近邻法","keywords":"","body":"K近邻法(KNN)k近邻模型模型距离度量k值的选择分类决策规则构建kd树搜索kd树概要 [TOC] K近邻法(KNN) k近邻法(k-nearest neighbor，k-NN)是一种基本分类与回归方法。 k近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。 k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻法不具有显式的学习过程。 k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。 k值的选择、距离度量及分类决策规则是k近邻法的三个基本要素。 k近邻法1968年由Cover和Hart提出。 输入：训练数据集 >T=\\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \\} > 其中，x_i \\in x \\subseteq R^n为实例的特征向量，y_i \\in Y =\\{ c_1,c_2,...,c_k \\} 为实例的类别，i＝1,2,...,N；实例特征向量x； 输出：实例x所属的类y。 根据给定距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的x的邻域记作N_{k}(x)； 在N_{k}(x)中根据分类决策规则(如多数表决)决定x的类别y： > y=\\arg \\max_{c_j}\\sum_{x_i \\in N_k(x)}{I(y_i=c_j)},i=1,2,...,N;j=1,2,...,K > 式（3.1）中，I为指示函数，即当y i ＝c j 时I为1，否则I为0。k近邻法的特殊情况是k＝1的情形，称为最近邻算法。对于输入的实例点（特征向量）x，最近邻法将训练数据集中与x最邻近点的类作为x的类。 k近邻模型 k近邻法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。 模型 k近邻法中，当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。 这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。这一事实从最近邻算法中可以看得很清楚。 特征空间中，对每个训练实例点ix，距离该点比其他点更近的所有点组成一个区域，叫作单元(cell)。 每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。 最近邻法将实例ix的类iy作为其单元中所有点的类标记(class label)。 这样，每个单元的实例点的类别是确定的。下图是二维特征空间划分的一个例子。 距离度量 特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是n维实数向量空间R^n 。 使用的距离是欧氏距离，但也可以是其他距离，如更一般的L_p距离(L_p pdistance)或Minkowski距离(Minkowski distance)。 设特征空间x是n维实数向量空间R^n ，x_i,x_j \\in X， x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T， x_j=(x_j^{(1)},x_j^{(2)},...,x_j^{(n)})^T，x_i，x_j的L_p距离定义为 >L_p(x_i,x_j)=(\\sum_{l=1}^{n}{|x_i^{(l)}-x_j^{(l)}|^{p}})^{\\frac{1}{p}} > 这里p \\geq 1。 当p＝2时，称为欧氏距离(Euclidean distance)，即 >L_2(x_i,x_j)=(\\sum_{l=1}^{n}{|x_i^{(l)}-x_j^{(l)}|^{2}})^{\\frac{1}{2}} > 当p＝1时，称为曼哈顿距离(Manhattan distance)，即 >L_p(x_i,x_j)=\\sum_{l=1}^{n}{|x_i^{(l)}-x_j^{(l)}|} > 当p＝\\infty时，它是各个坐标距离的最大值，即 >L_{\\infty}(x_i,x_j)=\\max_{l}{|x_i^{(l)}-x_j^{(l)}|} > 下图给出了二维空间中p取不同值时，与原点的L_p距离为1(L_p＝1)的点的图形。 k值的选择 k值的选择会对k近邻法的结果产生重大影响。 如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差(approximation error)会减小，只有与输入实例较近的(相似的)训练实例才会对预测结果起作用。 但缺点是“学习”的估计误差(estimation error)会增大，预测结果会对近邻的实例点非常敏感。 如果邻近的实例点恰巧是噪声，预测就会出错。 换句话说，k值的减小就意味着整体模型变得复杂，容易发生过拟合。 如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误。k值的增大就意味着整体的模型变得简单。 如果k＝N，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。 这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。 在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。 分类决策规则 k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。 多数表决规则(majority voting rule)有如下解释：如果分类的损失函数为0-1损失函数，分类函数为 >f:R^n \\to \\{ c_1,c_2,...,c_k \\} > 那么误分类的概率是 >P(Y \\neq f(X))=1-P(Y=f(X)) > 对给定的实例x \\in x，其最近邻的k个训练实例点构成集合N_k^{(x)}。如果涵盖N_k{(x)}的区域的类别是c_j ，那么误分类率是 >\\frac{1}{k}\\sum_{x_i \\in N_k{(x)}}I(y_i \\neq c_j)=1-\\frac{1}{k}\\sum_{x_i \\in N_k{(x)}}{}I(y_i=c_j) > 要使误分类率最小即经验风险最小，就要使\\sum_{x_i \\in N_k{(x)}}{}I(y_i=c_j)最大，所以多数表决规则等价于经验风险最小化。 构建kd树 实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。这点在特征空间的维数大及训练数据容量大时尤其必要。 k近邻法最简单的实现方法是线性扫描(linear scan)。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。 为了提高k近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。具体方法很多，下面介绍其中的kd树(kd tree)方法。 kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。 kd树是二叉树，表示对k维空间的一个划分(partition)。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。 构造kd树的方法如下： 构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域； 通过下面的递归方法，不断地对k维空间进行切分，生成子结点。 在超矩形区域(结点)上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域(子结点)；这时，实例被分到两个子区域。 这个过程直到子区域内没有实例时终止(终止时的结点为叶结点)。 在此过程中，将实例保存在相应的结点上。 通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数(median)为切分点，这样得到的kd树是平衡的。注意平衡的kd树搜索时的效率未必是最优的。 Example1: 给定一个二维空间的数据集： >T=\\{ (2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T \\} > 构造一个平衡kd树 解 　根结点对应包含数据集T的矩形，选择x^{(1)}轴，6个数据点的x^{(1)}坐标的中位数是7，以平面x^{(1)}=7将空间分为左、右两个子矩形(子结点)；接着，左矩形以x^{(2)}=4分为两个子矩形，右矩形以x^{(2)}=6分为两个子矩形，如此递归，最后得到下图所示的特征空间划分和下图所示的kd树。 搜索kd树 利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。这里以最近邻为例加以叙述，同样的方法可以应用到k近邻。 给定一个目标点，搜索其最近邻。首先找到包含目标点的叶结点；然后从该叶结点出发，依次回退到父结点；不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。 包含目标点的叶结点对应包含目标点的最小超矩形区域。 以此叶结点的实例点作为当前最近点。 目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部。 然后返回当前结点的父结点，如果父结点的另一子结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例点。 如果存在这样的点，将此点作为新的当前最近点。 算法转到更上一级的父结点，继续上述过程。 如果父结点的另一子结点的超矩形区域与超球体不相交，或不存在比当前最近点更近的点，则停止搜索。 Example1: 给定一个如图所示的kd树，根结点为A，其子结点为B，C等。 树上共存储7个实例点；另有一个输入目标实例点S，求S的最近邻。 解 　首先在kd树中找到包含点S的叶结点D(图中的右下区域)，以点D作为近似最近邻。 真正最近邻一定在以点S为中心通过点D的圆的内部。 然后返回结点D的父结点B，在结点B的另一子结点F的区域内搜索最近邻。 结点F的区域与圆不相交，不可能有最近邻点。 继续返回上一级父结点A，在结点A的另一子结点C的区域内搜索最近邻。 结点C的区域与圆相交；该区域在圆内的实例点有点E，点E比点D更近，成为新的最近邻近似。 最后得到点E是点S的最近邻。 概要 1．k近邻法是基本且简单的分类与回归方法。k近邻法的基本做法是：对给定的训练实例点和输入实例点，首先确定输入实例点的k个最近邻训练实例点，然后利用这k个训练实例点的类的多数来预测输入实例点的类。 2．k近邻模型对应于基于训练数据集对特征空间的一个划分。k近邻法中，当训练集、距离度量、k值及分类决策规则确定后，其结果唯一确定。 3．k近邻法三要素：距离度量、k值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的pL距离。k值小时，k近邻模型更复杂；k值大时，k近邻模型更简单。k值的选择反映了对近似误差与估计误差之间的权衡，通常由交叉验证选择最优的k。常用的分类决策规则是多数表决，对应于经验风险最小化。 4．k近邻法的实现需要考虑如何快速搜索k个最近邻点。kd树是一种便于对k维空间中的数据进行快速检索的数据结构。kd树是二叉树，表示对k维空间的一个划分，其每个结点对应于k维空间划分中的一个超矩形区域。利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/监督学习_支持向量机.html":{"url":"chapters/监督学习_支持向量机.html","title":"监督学习_支持向量机","keywords":"","body":"支持向量机(SVM)精简版(核方法) [TOC] 支持向量机(SVM) 精简版(核方法) 核方法的基本思想是通过一个非线性变换，把输入数据映射到高维的希尔伯特空间中，在这个高维空间里，那些在原始输入空间中线性不可分的问题变得更加容易解决，甚至线性可分。支持向量机(Support Vector Machine，SVM)是一类最典型的核方法，下面将以支持向量机为例，对核方法进行简单的介绍。 支持向量机的基本思想是通过核函数将原始输入空间变换成一个高维(甚至是无穷维)的空间，在这个空间里寻找一个超平面，它可以把训练集里的正例和负例尽最大可能地分开(用更加学术的语言描述，就是正负例之间的间隔最大化)。那么如何才能通过核函数实现空间的非线性映射呢？让我们从头谈起。 假设存在一个非线性映射函数\\phi，可以帮我们把原始输入空间变换成高维非线性空间。我们的目的是在变换后的空间里，寻找一个线性超平面w^T\\phi(x)=0，它能够把所有正例和负例分开，并且距离该超平面最近的正例和负例之间的间隔最大。这个诉求可以用数学语言表述如下： >max\\frac{2}{||w||} \\\\ >w^T\\phi(x_i) \\geq+1,如果y_i=+1 \\\\ >w^T\\phi(x_i) \\leq-1,如果y_i=-1 \\\\ >i=1,...,n > 其中\\frac{2}{||w||}是离超平面最近的正例和负例之间的间隔(如图2.6所示)。 ​ 以上的数学描述等价于如下的优化问题： >min \\frac{1}{2}||w||^2 \\\\ >y_i(w^T \\phi(x_i)) \\geq 1,i=1,...,n > 上式中的约束条件要求所有的正例和负例分别位于超平面w^T \\phi(x)=0的两侧。某些情况下，这种约束可能过强，因为我们所拥有的训练集有时是不可分的。这时候，就需要引入松弛变量\\xi，把上述优化问题改写为： >min \\frac{1}{2}||w||^2+C \\sum_i{\\xi_i} \\\\ >y_i(w^T \\phi(x_i)) \\geq 1- \\xi \\\\ >\\xi \\geq 0 >i=1,...,n > 其实这种新的表述等价于最小化一个加了正则项\\frac{1}{2}||w||^2l的Hinge损失函数。这是因为当1-y_i(w^T \\phi(x_i))小于0的时候，样本x_i被超平面正确地分到相应的类别里，\\xi_i =0；反之，\\xi_i将大于0，且是1-y_i(w^T \\xi (x_i)）的上界：最小化\\xi_i 就相应地最小化了y_i(w^T \\phi(x_i)) \\geq 1。基于以上讨论，其实支持向量机在最小化如下的目标函数： >\\hat l_n{(w)}=\\frac{1}{2}||w||^2+\\sum_{i=1}^{n}{max\\{0,1-y_i(w^T \\phi(x_i))\\}} > 其中，\\frac{1}{2}||w||^2是正则项，对它的最小化可以限制模型的空间，有效提高模型的泛化能力(也就是使模型在训练集和测试集上的性能更加接近)。 为了求解上述有约束的优化问题，一种常用的技巧是使用拉格朗日乘数法将其转换成对偶问题进行求解。具体来讲，支持向量机对应的对偶问题如下： >\\max_{\\alpha}{\\sum_{i=1}^{n}{\\alpha_i}}-\\frac{1}{2}\\sum_{i=1}^{n}{\\sum_{j=1}^{n}{\\alpha_i \\alpha_j y_i y_j \\phi(x_i)^T \\phi(x_j)}} \\\\ >s.t. \\sum_{i=1}^{n}{\\alpha_i y_i}=0,\\alpha_i \\geq 0 > 在对偶空间里，该优化问题的描述只与\\phi(x_i)和\\phi(x_j)的内积有关，而与映射函数\\phi本身的具体形式无关。因此，我们只需定义两个样本x_i和x_j之间的核函数k(x_i,x_j)，用以表征其映射到高维空间之后的内积即可： >k(x_i,x_j)=\\phi(x_i)^T\\phi(x_j) > 至此，我们弄清楚了核函数是如何和空间变换发生联系的。核函数可以有很多不同的选择，以下列出了几种常用的核函数。 核函数 数学形式 多项式和 k(x_i,x_j)=(x_i^Tx_j)^p,p \\geq1 高斯核 k(x_i,x_j)=exp(-\\frac{||x_i-x_j||^2}{2{\\sigma}^2}) 拉普拉斯核 k(x_i,x_j)=exp(-\\frac{||x_i-x_j||}{\\sigma}),\\sigma >0 Sigmoid核 k(x_i,x_j)=tanh(\\beta x_i^T x_j+\\theta),\\beta>0, \\theta 事实上，只要一个对称函数所对应的核矩阵满足半正定的条件，它就能作为核函数使用，并总能找到一个与之对应的空间映射。换言之，任何一个核函数都隐式地定义了一个再生核希尔伯特空间(Reproducing Kernel Hilbert Space，RKHS)。在这个空间里，两个向量的内积等于对应核函数的值。 > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/监督学习_CRF.html":{"url":"chapters/监督学习_CRF.html","title":"监督学习_CRF","keywords":"","body":"背景系列相关HMMMEMMCRF马尔科夫性团和最大团 [TOC] 一文理解条件随机场CRF 如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？ 机器学习-白板推导系列(十七)-条件随机场CRF（Conditional Random Field） 机器学习之CRF条件随机场 【NLP】从隐马尔科夫到条件随机场 13张动图，彻底看懂马尔科夫链、PCA和条件概率 背景 系列相关 分类问题根据是否引入概率分为了硬分类和软分类 SVM(几何间隔)、PLA(感知机)、LDA(类间大, 类内小) 概率判别模型 Logistic Regression (对$p(y|x)$建模，二分类)(多分类的话就是softmax) -> 都是最大熵模型(给定均值和方差，Gaussian Dist熵最大，做分类时可以看成对数线性模型) MEMM(打破了HMM的观测独立假设，但会引起Label bias Problem，原因是局部归一化引起的) CRF的提出就是为了解决MEMM的标注偏差问题(变成了无向图, 也就是全局归一化 ) 概率生成模型 Naive Bayesian(朴素贝叶斯二分类转序列的时候就是HMM，对$p(x,y)$建模) > p(x|y=1/0)=\\sum_{i=1}^{p}{p(x_i|y=0/1)} > Gaussian Mixture Model(加入时间的话就是HMM) Hidden Markor Model 1)齐次Markor 2)观测独立 HMM $\\lambda = (\\pi,A,B)$ 1)齐次Markor(链条的长度为1 给定3的时候 4和2条件独立) 齐次 是1-4的的转移概率是相同的 > p(y_t|y_{1:t-1},x_{1:t-1})=p(y_t|y_{t-1}) > 2)观测独立假设 > p(x_t|y_{1:t},x_{1:t-1})=p(x_t|y_t) > 建模对象: $P(X,Y|\\lambda)$ > P(X,Y|\\lambda)=\\prod_{t=1}^{T}{P(x_t,y_t|\\lambda)}=\\prod_{t=1}^{T}{P(y_t|y_{t-1},\\lambda)}P(x_t|y_t,\\lambda) > NB->HMM(观测独立假设不合理，例子 垃圾邮件分类，应该和每个单词都有关系) MEMM 建模对象$P(Y|X,\\lambda)$ > P(Y|X,\\lambda)=\\prod_{t=1}^{T}p(y_t|t_{t-1},x_{1:t},\\lambda) > 打破了观测独立假设 CRF 马尔科夫性 无向图随机变量之间满足三种性质 成对马尔科夫性 设$u$和$v$是无向图$G$中任意两个没有边连接的节点，节点$u$和$v$分别对应随机变量$Y_u$和$Y_v$。其他所有节点为$O$，对应的随机变量是$Y_o$。成对马尔可夫性是指给定随机变量组$Y_o$的条件下随机变量$Y_u$和$Y_v$是条件独立的，即 > P(Y_u,Y_v|Y_o)=P(Y_u|Y_o)P(Y_v|Y_o) > 局部马尔科夫性 设$v \\in V$是无向图$G$中任意一个节点，$W$是与$v$有边连接的所有节点，$O$是$v$、$W$以外的其他所有节点。$v$表示随机变量是$Y_v$，$W$表示的随机变量组是$Y_W$，$O$表示的随机变量组是$Y_o$。局部马尔可夫性是指在给定随机变量组$Y_W$的条件下随机变量$Y_v$与随机变量组$Y_o$是独立的，即 > P(Y_v,Y_o|Y_W)=P(Y_v|Y_W)P(Y_o|Y_W) > 全局成对马尔科夫性 设结点集$A$，$B$在无向图$G$中被结点集合$C$分开的任意结点的集合，具体如下图所示 则在给定了集合C的条件下结点集合A和B之间是相互独立的，具体表达式如下 > P(Y_A,Y_B|Y_C)=P(Y_A|Y_C)P(Y_B|Y_C) > 仔细观察发现这三种性质实质上是等价的，成对马尔科夫性和局部马尔科夫性都可以看作是全局马尔科夫性的特殊形式。 那这三种性质提出来有什么用呢？ 首先满足这三种性质的联合概率分布$P(Y)$可以称为马尔科夫随机场或者概率无向图模型。 而对于马尔科夫随机场，可以将联合概率分布$P(Y)$拆分成多个因子的乘积，这样就便于计算$P(Y)$。 团和最大团 团、极大团、最大团 团(clique)就是一个无向图的完全子图，既然是完全图，当然每对顶点之间都必须要有边相连。 团：无向图的完全子图。 完全图：完全图是一个简单的无向图，其中每对不同的顶点之间都恰连有一条边相连。 比如这里的{0、5}就是一个团，它就是原图的一个完全子图，并且结点之间连接，当然{0、4、5}，{1、2、4}同样也是团，团里面的结点都必须是互相连接的。还有许多的团并没有全部列举出来，比如{0、4}，{1、2}，{4、3}等等。 最大团就是就是结点数最多的极大团 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/非监督学习.html":{"url":"chapters/非监督学习.html","title":"非监督学习","keywords":"","body":"非监督学习 [TOC] 非监督学习 一文盘点6种聚类算法，数据科学家必备！ Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/非监督学习_K均值.html":{"url":"chapters/非监督学习_K均值.html","title":"非监督学习_K均值","keywords":"","body":"K均值(K-Means) [TOC] K均值(K-Means) > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/非监督学习_Mean Shift均值漂移聚类.html":{"url":"chapters/非监督学习_Mean Shift均值漂移聚类.html","title":"非监督学习_Mean Shift均值漂移聚类","keywords":"","body":"均值漂移聚类(Mean Shift) [TOC] 均值漂移聚类(Mean Shift) > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/非监督学习_DBSCAN基于密度的聚类方法.html":{"url":"chapters/非监督学习_DBSCAN基于密度的聚类方法.html","title":"非监督学习_DBSCAN基于密度的聚类方法","keywords":"","body":"基于密度的聚类方法(DBSCAN) [TOC] 基于密度的聚类方法(DBSCAN) > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/非监督学习_Hierarchical Clustering层次聚类.html":{"url":"chapters/非监督学习_Hierarchical Clustering层次聚类.html","title":"非监督学习_Hierarchical Clustering层次聚类","keywords":"","body":"层次聚类(Hierarchical Clustering) [TOC] 层次聚类(Hierarchical Clustering) > > > Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/非监督学习_Spectral Clustering谱聚类.html":{"url":"chapters/非监督学习_Spectral Clustering谱聚类.html","title":"非监督学习_Spectral Clustering谱聚类","keywords":"","body":"谱聚类(Spectral Clustering) [TOC] 谱聚类(Spectral Clustering) 白话什么是谱聚类算法 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/半监督学习.html":{"url":"chapters/半监督学习.html","title":"半监督学习","keywords":"","body":"半监督学习 [TOC] 半监督学习 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/集成学习.html":{"url":"chapters/集成学习.html","title":"集成学习","keywords":"","body":"集成学习 [TOC] 集成学习 Bagging和Boosting的区别（面试准备） Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "},"chapters/强化学习.html":{"url":"chapters/强化学习.html","title":"强化学习","keywords":"","body":"强化学习 [TOC] 强化学习 Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook该文件修订时间： 2022-02-07 05:34:22 new Valine({el: \"#vcomments\",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false}) "}}