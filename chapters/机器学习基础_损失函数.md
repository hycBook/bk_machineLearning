![异世界.png](./res/other/异世界蕾姆_1.png)

# 损失函数

# 经验误差风险

>假定我们的目的是学习一个模型，用以自动判断某产品评论是正面的还是负面的。
>
>为了更好地对机器学习的基本流程进行描述，我们首先对有监督的二分类问题进行数学建模。假设我们有一个训练数据集，包含$$n$$个样本$$\{x_i\}_{i=1}^n$$，每个样本$$x$$；可以表示成一个$$d$$维的向量：$$x_i \in X \subseteq R^d$$。样本$$x_{i}$$被赋予了一个标签$$y_i$$，表征该样本属于正类还是负类：$$y_i \in Y= \{+1,-1\}$$。
>
>假设我们最终想要通过机器学习获得一个分类模型$$g:X \to R$$，它以$$X$$空间内任意的$$d$$维向量为输入，通过一个由参数$$w$$驱动的变换，输出一个分数，然后取这个分数的符号，得到$$Y$$空间的预测标签：$$sgn(g(x_i;w))$$。那么现在的问题是：什么样的分类模型才是好的？如何才能学到一个好的分类模型呢？
>    可以根据一个分类模型$$g$$在训练集上的表现来评价它的好坏。换言之，我们把$$g$$作用在每一个训练样本$$x_i$$；上，获得相应的输出值$$g(x_i;w)$$，然后把这个输出值与$$x_i$$本身的类别标签$$y_i$$，进行比对，如果二者相同就说明$$g$$在这个样本上实现了正确的分类，否则就判定它分类错误。这个判定可以用一个简单的示性误差函数加以表示：
>$$
>\epsilon(w;x_i,y_i)=1_{|y_ig(x_i,w)|<0}
>$$
>​	如果分类模型$$g$$把训练集里所有的样本或绝大部分样本都分到了正确的类别里，我们就说它是一个好的分类器；相反，如果$$g$$在很多样本上都做出了错误的判断，我们就说它不是一个好的分类器。这种定性的判断可以用一个称为`经验误差风险`的数值来进行定量衡量，也就是分类模型$$g$$在所有的训练样本上所犯错误的总和：
>$$
>\hat{\epsilon}(w)=\sum_{i=1}^{n}{1_{|y_ig(x_i,w)|<0}}
>$$
>​	如果$$\hat{\epsilon}(w)**$$**为0或者取值很小，我们就说$$g$$的经验误差风险很小，是一个不错的分类模型。反之，如果$$\hat{\epsilon}(w)$$很大，则对应的经验误差风险很大，$$g$$就不是一个好的分类模型。
>​    通常，我们会通过在训练集上最小化经验误差风险来训练分类模型。换言之，通过调节$$g$$的参数$$w$$，使得经验误差风险。$$\hat{\epsilon}(w)$$不断下降，最终达到最小值的时候，我们就获得了一个所谓“最优”的分类模型。这件事说起来容易，实操起来还是有难度的，主要的问题出在。$$\hat{\epsilon}(w)$$的数学性质上。按照上面的定义，$$\hat{\epsilon}(w)$$是一组示性函数的和，因此是一个不连续、不可导的函数，不易优化。
>
>​	为了解决这个问题，人们提出了`损失函数`的概念。所谓的损失函数就是和误差函数有一定的关系(例如是误差函数的上界)但具有更好的数学性质(比如连续、可导、凸性等)，比较容易进行优化。通过对经验损失风险的最小化，我们可以间接地实现对经验误差风险$$\hat{\epsilon}(w)$$的最小化。为了便于引用，我们用$$\hat{l}(w)$$来表示经验损失风险。
>​    因为损失函数满足了连续可导的条件，所以在优化过程中选择面就比较宽了，有很多方法可供使用。我们既可以选择确定性的优化算法(包含以梯度下降法、坐标下降法为代表的一阶算法，以及以牛顿法、拟牛顿法为代表的二阶算法)，也可以选择随机性的优化算法(包括随机梯度下降法、随机坐标下降法、随机拟牛顿法等)。
>
>​	当优化算法收敛以后，我们就得到了一个不错的模型。当然，这个“不错”的模型到底能有多好还要看损失函数的复杂程度。如果损失函数是个凸函数，则很容易通过上述方法找到全局最优模型；否则，多数情况下我们得到的只是局部最优模型。无论是哪种情况，未来我们将会使用这个学到的模型对未知的新样本进行分类。
>
>![1571560658397](res/Machine%20Learning%20Base/1571560658397.png)

# 损失函数定义

>在二分类问题中，$$0-1$$误差是最终的评价准则，但是因为它不是一个连续的凸函数，直接用它来指导模型优化的过程未必是一个好的选择。为了解决这个问题，人们通常使用损失函数作为$$0-1$$误差的一个凸近似或者凸上界，然后通过最小化损失函数，来间接地达到最小化$$0-1$$误差的目的。本节将介绍几种典型的损失函数。
>
>`Hinge损失函数`衡量的是预测模型的输出的符号和类别标签的符号是否一致以及一致的程度。
>
>其具体数学形式如下：
>$$
>l(w;x,y)=max\{0,1-yg(x;w)\}
>$$
>​	从以上数学定义可以看出：当$$g(x;w)$$和$$y$$符号相同且乘积数值超过1时，损失函数取值为0；否则，将有一个线性的损失(二者符号不同时，乘积的绝对值越大，损失越大)。Hinge损失函数是一个连续凸函数，但是它在0点不可导，人们通常会选择次导数集合中的任意一个数值参与优化过程。我们从图2.3可以清晰地看出，Hinge损失是$$0-1$$误差的上界，因此通过最小化Hinge损失，可以有效地减小$$0-1$$误差，从而提高分类性能。
>
>![1571560745676](res/Machine%20Learning%20Base/1571560745676.png)`指数损失函数`指数损失函数也是$$0-1$$误差的上界，它的具体形式如下（参见图2.4）：
>$$
>l(x;x,y)=exp(-yg(x:w))
>$$
>从以上定义可以看出，指数损失函数对于预测模型输出的符号与类别标签的符号不一致的情况有强烈的惩罚，相反，当二者符号一致且乘积数值较大时，损失函数的取值会非常小。指数损失函数的基本形状和Hinge损失函数很接近，只不过它对于符号不一致的情况的惩罚力度更大(指数力度vs.线性力度)，而且它是全程连续可导的凸函数，对于优化过程更加有利。
>
>`交叉熵损失函数`也是常用的损失函数之一，它假设预测模型以下述形式决定了标签的概率分布：
>$$
>P(Y=1|x;w)=\frac{exp(g(x;w))}{exp(g(x;w))+exp(-g(x;w))}
>$$
>并且试图衡量该概率与标签之间的差别。其数学定义如下（参见图2.5）：
>$$
>l(w;x,y)-\sum_z \in \{-1,1 \}{I_{|y=z|}}{}{logP(Y=z|x;w)}
>$$
>可见，最小化交叉熵损失函数等价于最大化预测函数g所对应的条件似然函数。
>
>![1571566807764](res/Machine%20Learning%20Base/1571566807764.png)
>
>从以上定义可以看出，对于正类的样本而言，当12预测模型的输出接近于1时，损失很小；而当预测模型的输出接近于0时，则产生一个很大的损失。相反，对于负类的样本而言，当预测模型的输出接近于1时，会产生很大的损失；而当预测模型的输出接近于0时，则损失很小。交叉熵损失函数也是一个全程连续可导的凸函数，并且是$$0-1$$误差的上界。图2.5交叉熵损失函数以上介绍了一些常用的损失函数。虽然它们和$$0-1$$误差在形式上有所差别，但是从统计意义上讲，它们存在着很强的关联关系。可以证明，在一定假设下，以上损失函数对于$$0-1$$误差而言都具有统计一致性，也就是说，当样本趋近于无穷多的时候，按照最小化损失函数找到的最优模型也是在$$0-1$$误差意义下的最优模型。这就给使用这些损失函数奠定了理论基础。
>
>

# 模型泛化误差

>机器学习算法的最终目标是最小化期望损失风险(也就是模型在任意未知测试样本上的表现)：
>$$
>\min_{g \in G}L(g)=E_{x,y \sim  P_{x,y}}l(g;x,y)
>$$
>其中$$G$$是一个预先给定的函数族。
>	由于数据的真实分布$$P_{x,y}$$.通常是不知道的，我们的可用信息来自于训练数据$$S_n=\{(x_1,y_1),...,(x_n,y_n)\}$$。因此，我们的学习目标转化为最小化经验风险：
>$$
>\min_{g \in G}{\hat l(g)}=\frac{1}{n} \sum_{i=1}^{n}{l(g;x_i,y_i)}
>$$
>当函数空间$$G$$受限时，比如我们只允许优化算法在那些范数小于$$c$$的函数子空间里进行搜索，亦即$$G_c=\{g:g \in G, ||g||_G \leq c\}$$，我们称相应的学习问题为正则经验风险最小化。
>优化算法对(正则化)经验风险最小化问题进行求解，并在算法结束的第$$T$$次迭代中输出模型$$\hat g_r$$。我们希望所学习到的模型$$\hat g_r$$的期望风险$$L(\hat g_r)$$尽可能小，并将其定义为机器学习算法的泛化误差。
>
>



