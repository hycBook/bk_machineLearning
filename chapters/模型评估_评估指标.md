![异世界.png](./res/other/异世界蕾姆_1.png)

[TOC]



# 评估指标

>[不可不知的11个重要机器学习模型评估指标](不可不知的11个重要机器学习模型评估指标)
>
>

# 准确率(Accuracy)

>`准确率`是指分类正确的样本占总样本个数的比例，即
>$$
>Accurcy=\frac{n_{correct}}{n_{total}}=\frac{TP+TN}{TP+TN+FP+FN}
>$$
>其中$$n_{correct}$$为被正确分类的样本个数，$$n_{total}$$为总样本的个数。
>准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。
>所以，当不同类别的样本比例非常不均衡时， 占比大的类别往往成为影响准确率的最主要因素。
>
>`错误率`是指分类错误的样本数占样本总数的比例，即
>$$
>ErrorRate=1-Accurcy=\frac{FP+FN}{TP+TN+FP+FN}
>$$
>

# 精确度(Precision)

>`精确率`是指分类正确的正样本个数占分类器判定为正样本的样本个数的比例。
>`召回率`是指分类正确的正样本个数占真正的正样本个数的比例。
>在排序问题中，通常没有一个确定的闽值把得到的结果直接判定为正样本或负样本，而是采用$$TopN$$返回结果的$$Precision$$值和$$Recall$$值来衡量排序模型的性能，即认为模型返回的$$Top N$$的结果就是模型判定的正样本，然后计算前$$N$$个位置上的准确率$$Precision@N$$和前$$N$$个位置上的召回率$$Recall@N$$。
>
>|                                 | 相关(Relevant)，正类                                         | 无关(NonRelevant)，负类                                      |
>| ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
>| 被检索到<br />(Retrieved）      | true positives(`TP`正类判定为正类，例子中就是正确的判定“这位是女生") | false positives(`FP`负类判定为正类，“存伪"，例子中就是分明是男生却判断为女生，当下伪娘横行，这个错常有人犯) |
>| 未被检索到<br />(not Retrieved) | false negatives(`FN`正类判定为负类，“去真”，例子中就是，分明是女生，这哥们却判断为男生--梁山伯同学犯的错就是这个） | true negatives(`TN`负类判定为负类，也就是一个男生被判断为男生， 像我这样的纯爷们一准儿就会在此处） |
>
>根据这四种数据，有四个比较重要的比率：
>$$
>True Positive Rate(真正率TPR或灵敏度sensitivity)=TPR=\frac{TP}{TP+FN}=\frac{正样本预测结果数}{正样本实际数} \\
>True Negative Rate(真负率TNR或特指度specificity)=TNR=\frac{TN}{TN+FP}=\frac{负样本预测结果数}{负样本实际数} \\
>False Positive Rate(假正率FPR)=FPR=\frac{FP}{FP+TN}=\frac{被预测为正的负样本结果数}{负样本实际数} \\
>False Negative Rate(假负率FNR)=FNR=\frac{FN}{TP+FN}=\frac{被预测为负的正样本结果数}{正样本实际数} \\
>$$
>`灵敏度`表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力：
>$$
>Sensitive = \frac{TP}{P}
>$$
>`特效度`表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力：
>$$
>specificity = \frac{TN}{N}
>$$
>`精确率`(precision)计算的是所有“正确被检索的结果（TP）“占所有“实际被检索到的$$(TP+FP)$$"的比例：
>$$
>P=\frac{TP}{TP+FP}
>$$
>在例子中就是希望知道此君得到的所有人中，正确的人(也就是女生)占有的比例，所以其$$precision$$也就是40%($$\frac{20女生}{(20女生+30误判为女生的男生)}$$).

# 召回率(Recall)

>`召回率`(recall)的公式是$$R=\frac{TP}{TP+FN}$$，它计算的是所有“正确被检索的结果(TP)“占所有“应该检索到的结果$$(TP+FN)$$"的比例.
>在例子中就是希望知道此君得到的女生占本班中所有女生的比例，所以其$$recall$$也就是100%($$\frac{20女生}{(20女生+0误判为男生的女生)}$$）
>
>

# 均方根误差(RMSE)

>`概念`: RMSE经常被用来衡量回归模型的好坏，计算公式为
>$$
>RMSE=\sqrt{\frac{\sum_{i=1}^{n}{(y_i-\hat y)^2}}{n}}
>$$
>其中，$$y_i$$ 是第$$i$$个样本点的真实值，$$\hat y$$ 是第$$i$$个样本点的预测值，$$n$$是样本点的个数。
>
>​    一般情况下，$$RMS$$E能够很好地反映回归模型预测值与真实值的概念偏离程度。
>
>`缺点`: 但在实际问题中，如果存在个别偏离程度非常大的离群点(Outlier)时，即使离群点数量非常少，也会让$$RMSE$$指标变得很差。
>    回到问题中来，模型在95%的时间区间内的预测误差都小于1%， 这说明在大部分时间区间内，模型的预测效果都是非常优秀的。然而$$RMSE$$却一直很差，这很可能是由于在其他的5%时间区间内存在非常严重的离群点。
>
>事实上，在流量预估这个问题中，噪声点确实是很容易产生的，比如流量特别小的美剧、刚上映的美剧或者刚获奖的美剧，甚至一些相关社交媒体突发事件带来的流量，都可能会造成离群点。
>
>`解决`: 可以从三个角度来思考。
>
>第一， 如果我们认定这些离群点是“噪声点”的话，就需要在数据预处理的阶段把这些噪声点过滤掉。
>
>第二，如果不认为这些离群点是“噪声点”的 话，就需要进一步提高模型的预测能力，将离群点产生的机制建模进去(这是一个宏大的话题，这里就不展开讨论了)。
>
>第三，可以找一个更合适的指标来评估该模型。
>
>关于评估指标，其实是存在比$$RMSE$$的鲁棒性更好的指标，比如`平均绝对百分比误差`(Mean Absolute Percent Error，MAPE)，它定义为 
>$$
>MAPE=\sum_{i=1}{n}{|\frac{y_i-\hat y}{y_i}|}\times \frac{100}{n}
>$$
>相比$$RMSE$$，$$MAPE$$相当于把每个点的误差进行了归一化，降低了个别离群点带来的绝对误差的影响。
>
>

# F1得分

>除此之外，$$F1score$$和$$ROC$$曲线也能综合地反映一个排序模型的性能。
>
>$$F1score$$是精准率和召回率的调和平均值，它定义为
>$$
>F1=\frac{2 \times precision \times recall}{precision + recall}
>$$
>

# P-R曲线

>为了综合评估一个排序模型的好坏，不仅要看模型在不同$$TopN$$下的$$Precision@N$$和$$Recall@N$$，而且最好绘制出模型的$$P-R$$(Precision-Recall)曲线。
>
>$$P-R$$曲线的`横轴是召回率，纵轴是精确率`。
>
>对于一个排序模型来说， 其$$P-R$$曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。
>
>整条$$P-R$$曲线是通过将阈值从高到低移动而生成的。
>
>图是$$P-R$$曲线样例图，其中实线代表模型$$A$$的$$P-R$$曲线，虚线代表模型$$B$$的$$P-R$$曲线。
>
>![1569979463652](res/Machine%20Learning%20Base/1569979463652.png)
>
>原点附近代表当阀值最大时模型的精确率和召回率。
>
>

# ROC曲线

## ROC曲线概念

>`ROC曲线`是Receiver Operating Characteristic Curve的简称， 中文名为“受试者工作特征曲线”。
>
>$$ROC$$曲线源于军事领域，而后在医学领域应用甚广，“受试者工作特征曲线”这一名称也正是来自医学领域。
>
>$$ROC$$曲线的`横坐标为假阳性率`(False Positive Rate, $$FPR$$)；`纵坐标为真阳性率`(True Positive Rate, $$TPR$$)。$$FPR$$和$$TPR$$的计算方法分别为
>$$
>FPR=\frac{FP}{N} ,TPR=\frac{TP}{P}
>$$
>式中，$$P$$是真实的正样本的数量，$$N$$是真实的负样本的数量，$$TP$$是$$P$$个正样本中被分类器预测为正样本的个数，$$FP$$是$$N$$个负样本中被分类器预测为正样本的个数。
>
>
>
>假设有10位疑似癌症患者，其中有3位很不幸确实患了癌症($$P=3$$)，另外7位不是癌症患者($$N=7$$)。医院对这10位疑似患者做了诊断，诊断出3位癌症患者，其中有2位确实是真正的患者($$TP=2$$)。
>
>那么真阳性率$$TPR=\frac{TP}{P}=\frac{2}{3}$$。对于7位非癌症患者来说，有一位很不幸被误诊为癌症患者($$FP=1$$)，那么假阳性率$$FPR=\frac{FP}{N}=\frac{1}{7}$$。
>
>对于“该医院”这个分类器来说，这组分类结果就对应ROC曲线上的一个点($$\frac{1}{7}, \frac{2}{3}$$）。
>事实上，ROC曲线是通过`不断移动分类器的“截断点”来生成曲线上的一组关键点`的。
>
>通过下面的例子进一步来解释“截断点”的 概念。
>在二值分类问题中，模型的输出一般都是预测样本为正例的概率。
>
>| 样本序号 | 真实标签 | 模型输出概率 | 样本序号 | 真实标签 | 模型输出概率 |
>| -------- | -------- | ------------ | -------- | -------- | ------------ |
>| 1        | p        | 0.9          | 11       | p        | 0.4          |
>| 2        | p        | 0.8          | 12       | n        | 0.39         |
>| 3        | n        | 0.7          | 13       | p        | 0.38         |
>| 4        | p        | 0.6          | 14       | n        | 0.37         |
>| 5        | p        | 0.55         | 15       | n        | 0.36         |
>| 6        | p        | 0.54         | 16       | n        | 0.35         |
>| 7        | n        | 0.53         | 17       | p        | 0.34         |
>| 8        | n        | 0.52         | 18       | n        | 0.33         |
>| 9        | p        | 0.51         | 19       | p        | 0.30         |
>| 10       | n        | 0.505        | 20       | n        | 0.1          |
>
>假设测试集中有20个样本，表2.1是模型的输出结果。样本按照预测概率从高到低排序。在输出最终的正例、负例之前，我们需要指定一个 阈值，预测概率大于该阀值的样本会被判为正例，小于该阈值的样本则会被判为负例。
>
>比如，指定阀值为0.9，那么只有第一个样本会被预测为正例，其他全部都是负例。
>
>上面所说的“截断点”指的就是区分正 预测结果的阈值。
>通过动态地调整截断点，从最高的得分开始(实际上是从正无穷开始，对应着$$ROC$$曲线的零点)，逐渐调整到最低得分，每一个截断点都会对应一个$$FPR$$和$$TPR$$，在$$ROC$$图上绘制出每个截断点对应的位置， 再连接所有点就得到最终的$$ROC$$曲线。
>
>

## 绘制ROC曲线

>就本例来说，当截断点选择为正无穷时，模型把全部样本预测为负例，那么$$FP$$和$$TP$$必然都为0，$$FPR$$和$$TPR$$也都为0，因此曲线的第 一个点的坐标就是$$(0,0)$$。
>
>当把截断点调整为0.9时，模型预测1号样本为正样本，并且该样本确实是正样本，因此$$TP=1$$，20个样本中， 所有正例数量为$$P=10$$，故$$TPR=\frac{TP}{P}=\frac{1}{10}$$；
>
>这里没有预测错的正样本， 即$$FP=0$$，负样本总数$$N=10$$，故$$FPR=\frac{FP}{N}=\frac{0}{10}=0$$，对应ROC曲线上的点$$(0,0.1)$$。
>
>依次调整截断点，直到画出全部的关键点，再连接关键点即得到最终的$$ROC$$曲线，如下图所示。
>
>![1569981579326](res/Machine%20Learning%20Base/1569981579326.png)
>
>其实，还有一种更直观地绘制$$ROC$$曲线的方法。
>
>1. 首先，根据样本 标签统计出正负样本的数量，假设正样本数量为$$P$$，负样本数量为$$N$$； 
>
>2. 接下来，把横轴的刻度间隔设置为$$1/N$$，纵轴的刻度间隔设置为$$1/P$$； 
>
>3. 再根据模型输出的预测概率对样本进行排序（从高到低）；
>
>4. 依次遍历样 本，同时从零点开始绘制$$ROC$$曲线，每遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在$$(1,1)$$这个点， 整个$$ROC$$曲线绘制完成。
>
>

## ROC和P-R曲线

>![1569981858984](res/Machine%20Learning%20Base/1569981858984.png)
>
>![1569981873028](res/Machine%20Learning%20Base/1569981873028.png)
>
>可以看出，$$P-R$$曲线发生了明显的变化，而$$ROC$$曲线形状基本不变。
>
>这个特点让$$ROC$$曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。
>
>这有什么实际意义呢？在很多实际问题中，正负样本数量往往很不均衡。比如，计算广告领域经常涉及转化率模型，正样本的数量往往是负样本数量的1/1000甚至1/10000。
>
>若选择不同的测试集，$$P-R$$曲线的变化就会非常大，而$$ROC$$曲线则能够更加稳定地反映模型本身的好坏。
>
>所以，$$ROC$$曲线的适用场景更多，被广泛用于`排序、推荐、广告`等领域。
>
>但需要注意的是，选择$$P-R$$曲线还是$$ROC$$曲线是因实际问题而异的，如果研究者希望更多地看到模型在特定数据集上的表现，$$P-R$$曲线则能够更直观地反映其性能。
>
>

# AUC曲线

> 顾名思义，$$AUC$$指的是$$ROC$$曲线下的面积大小，该值能够量化地反映基于$$ROC$$曲线衡量出的模型性能。
>
>计算$$AUC$$值只需要沿着$$ROC$$横轴做积分就可以了。
>
>由于$$ROC$$曲线一般都处于$$y=x$$这条直线的上方(如果不是的话，只要把模型预测的概率反转成$$1-p$$就可以得到一个更好的分类器)，所以$$AUC$$的取值一般在$$[0.5,1]$$之间。
>
>$$AUC$$越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。
>
>

# 聚类指标

>一个好的聚类方法可以产生高品质簇，是的簇内相似度高，簇间相似度低。一般来说，评估聚类质量有两个标准，内部质量评价指标和外部评价指标。 
>
>##### 3.1.9.1 内部质量评价标准
>
>内部评价指标是利用数据集的属性特征来评价聚类算法的优劣。通过计算总体的相似度，簇间平均相似度或簇内平均相似度来评价聚类质量。评价聚类效果的高低通常使用聚类的有效性指标，所以目前的检验聚类的有效性指标主要是通过簇间距离和簇内距离来衡量。这类指标常用的有CH(Calinski-Harabasz)指标等 
>
>`CH指标`：
>$$
>CH(K)=\frac{tr(B)/(K-1)}{tr(W)/(N-K)} \\
>tr(B)=\sum_{j=1}^{k}{||z_j-z||^2} \\
>tr(W)=\sum_{j=1}^{k}{\sum_{x_i \in z_k}{||x_i-z_j||^2}}
>$$
>其中$$tr(B)$$表示类间距离差矩阵的迹，$$tr(W)$$表示类内差矩阵的迹，$$z$$是整个数据集的均值，$$z_j$$是第$$j$$个簇$$c_j$$的均值，$$N$$代表聚类的个数， $$K$$代表当前的类。值越大，$$CH(K)$$聚类效果越好，$$CH$$主要计算簇间距离与簇内距离的比值
>
>`簇的凝聚度`：
>$$
>SSE=\sum_{i=1}^{r}{\sum_{j=1}^{n_i}{(X_{ij}-\bar X_i)^2}} \\
>\bar X_i=\frac{1}{n_i}\sum_{j=1}^{n_i}{X_{ij}},i=1,2,...,r
>$$
> 簇内点对的平均距离反映了簇的凝聚度，一般使用组内误差平方(SSE)表示
>
>`簇的邻近度`：
>
>簇的邻近度用组间平方和(SSB)表示，即簇的质心$$C_i$$到簇内所有数据点的总平均值$$c$$的距离的平方和 
>
>##### 3.1.9.2 外部质量评价标准
>
>外部质量评价指标是基于已知分类标签数据集进行评价的，这样可以将原有标签数据与聚类输出结果进行对比。
>
>外部质量评价指标的理想聚类结果是：具有不同类标签的数据聚合到不同的簇中，具有相同类标签的数据聚合相同的簇中。外部质量评价准则通常使用熵，纯度等指标进行度量。 
>
>`熵`：
>
> 簇内包含单个类对象的一种度量。对于每一个簇，首先计算数据的类分布，即对于簇$$i$$，计算簇$$i$$的成员属于类$$j$$的概率
>$$
>p_{ij}=\frac{m_{ij}}{m_i}
>$$
> 其中$$m_i$$表示簇$$i$$中所有对象的个数，而$$m_{ij}$$是簇$$i$$中类$$j$$的对象个数。使用类分布，用标准公式：
>$$
>e_i=-\sum_{j=1}^{K}{p_{ij}log_2{p_ij}}
>$$
> 计算每个簇$$i$$的熵，其中$$K$$是类个数。簇集合的总熵用每个簇的熵的加权和计算即：
>$$
>e=\sum_{i=1}^{K}{\frac{m_i}{m}e_i}
>$$
> 其中$$K$$是簇的个数，而$$m$$是簇内数据点的总和
>
>`纯度`：
>
> 簇内包含单个类对象的另外一种度量。簇$$i$$的纯度为$$p_i=\max_{a}{p_{ij}}$$，而聚类总纯度为： 
>$$
>purity=\sum_{i=1}^{K}{\frac{m_i}{m}p_i}
>$$
>[08 聚类算法 - 聚类算法的衡量指标](https://www.jianshu.com/p/1049db259d38)
>
>[机器学习（7）——聚类算法](https://www.jianshu.com/p/2fa67f9bad60)

# 回归指标

>SSE(和方差、误差平方和)：The sum of squares dueto error
>MSE(均方差、方差)：Meansquared error
>RMSE(均方根、标准差)：Root mean squared error
>R-square(确定系数)：Coefficientof determination
>Adjusted R-square：Degree-of-freedomadjusted coefficient of determination
>
>`SSE`：
>$$
>SSE = \sum(Y_{actual} - Y_{predict})^2
>$$
>`R-square`：
>$$
>R^2 = 1-\frac{\sum(Y_{actual}-Y_{predict})^2}{\sum(Y_{actual}-Y_{mean})^2}=1-\frac{SSE}{SST} \\
>SST = \sum_{i=1}^{n}w_i(Y_{actual} - Y_{mean})^2
>$$
>`Adjusted R-square`：
>$$
>R^2 _{adjusted}=1-\frac{(1-R^2)(n-1)}{n-p-1}
>$$
>其中$$n$$是样本数量；$$p$$是特征数量；$$R^2$$是决定系数，该指标消除了样本数量和特征数量的影响，做到了真正的0~1，越大越好。
>
>`AIC赤池信息准则`：
>$$
>AIC=e^\frac{2k}{T}\frac{\sum_{t=1}^{T}e_t^2}{T} \\
>e^\frac{2k}{T}:惩罚因子
>$$
>估计模型拟合数据的优良性，AIC值越小说明模型拟合得越好
>
>[AIC和BIC准则](AIC和BIC准则)
>
>[模型选择方法：AIC和BIC](https://www.jianshu.com/p/4c8cf5df2092)

