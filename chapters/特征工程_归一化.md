![异世界.png](https://upload-images.jianshu.io/upload_images/15675864-e39212ac990782cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

[TOC]

# 归一化

>为什么需要对数值型特征做归一化呢？
>
>我们不妨借助随机梯度下降的实例来说明归一化的重要性。假设有两种数值型特征，$$x_1$$的取值范围为$$[0,10]$$，$$x_2$$的取值范围为$$[0,3]$$，于是可以构造一个目标函数符合图1.1(a)中的等值图。
>
>![1570006953459](res/Machine%20Learning%20Base/1570006953459.png)
>
>​    在学习速率相同的情况下，$$x_1$$的更新速度会大于$$x_2$$，需要较多的迭代才能找到最优解。
>
>如果将$$x_1$$和$$x_2$$归一化到相同的数值区间后，优化目标的等值图会变成图1.1(b)中的圆形，$$x_1$$和$$x_2$$的更新速度变得更为一致，容易更快地通过梯度下降找到最优解。
>当然，数据归一化并不是万能的。在实际应用中，`通过梯度下降法求解的模型通常是需要归一化的`，包括线性回归、逻辑回归、支持向 量机、神经网络等模型。
>
>但对于决策树模型则并不适用，以C4.5为例， 决策树在进行节点分裂时主要依据数据集$$D$$关于特征$$x$$的信息增益比，而信息增益比跟特征是否经过归一化是无关的， 因为归一化并不会改变样本在特征$$x$$上的信息增益。
>
>

# 线性函数归一化

>`线性函数归一化`(Min-Max Scaling): 它对原始数据进行线性变换，使结果映射到$$[0,1$$]的范围，实现对原始数据的等比缩放。
>$$
>X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}
>$$
>其中$$X$$为原始数据，$$X_{max}$$、$$X_{min}$$分别为数据最大值和最小值。

# 零均值归一化

>`零均值归一化`(Z-Score Normalization): 将原始数据映射到均值为0、标准差为1的分布上。
>
>具体来说，假设原始特征的均值为$$\mu$$、标准差为$$\sigma$$，那么归一化公式定义为
>$$$$
>z=\frac{x-\mu}{\sigma}
>$$$$

# 其他归一化方式

>
>
>









