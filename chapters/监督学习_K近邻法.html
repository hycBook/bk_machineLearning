<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>监督学习_K近邻法 · 机器学习相关学习记录</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-change_girls/girls.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="监督学习_支持向量机.html" rel="next"/>
<link href="监督学习_决策树.html" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../https:/upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png" rel="shortcut icon" type="image/x-icon"/>
<link href="../https:/upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png" rel="bookmark" type="image/x-icon"/>
<link href="../https:/upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png" rel="apple-touch-icon"/>
<link href="../https:/upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png" rel="apple-touch-icon" sizes="120x120"/>
<link href="../https:/upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png" rel="apple-touch-icon" sizes="180x180"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"narutohyc","repo":"bk_machineLearning","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="输入并搜索" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://github.com/narutohyc" target="_blank">我的狗窝</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter" data-level="1.2" data-path="机器学习基础.html">
<a href="机器学习基础.html">
<b>1.2.</b>
                    
                    机器学习基础
            
                </a>
<ul class="articles">
<li class="chapter" data-level="1.2.1" data-path="机器学习基础_距离.html">
<a href="机器学习基础_距离.html">
<b>1.2.1.</b>
                    
                    机器学习基础_距离
            
                </a>
</li>
<li class="chapter" data-level="1.2.2" data-path="机器学习基础_概率论基础.html">
<a href="机器学习基础_概率论基础.html">
<b>1.2.2.</b>
                    
                    机器学习基础_概率论基础
            
                </a>
</li>
<li class="chapter" data-level="1.2.3" data-path="机器学习基础_线性代数基础.html">
<a href="机器学习基础_线性代数基础.html">
<b>1.2.3.</b>
                    
                    机器学习基础_线性代数基础
            
                </a>
</li>
<li class="chapter" data-level="1.2.4" data-path="机器学习基础_微积分基础.html">
<a href="机器学习基础_微积分基础.html">
<b>1.2.4.</b>
                    
                    机器学习基础_微积分基础
            
                </a>
</li>
<li class="chapter" data-level="1.2.5" data-path="机器学习基础_最优化理论.html">
<a href="机器学习基础_最优化理论.html">
<b>1.2.5.</b>
                    
                    机器学习基础_最优化理论
            
                </a>
</li>
<li class="chapter" data-level="1.2.6" data-path="机器学习基础_损失函数.html">
<a href="机器学习基础_损失函数.html">
<b>1.2.6.</b>
                    
                    机器学习基础_损失函数
            
                </a>
</li>
</ul>
</li>
<li class="chapter" data-level="1.3" data-path="特征工程.html">
<a href="特征工程.html">
<b>1.3.</b>
                    
                    特征工程
            
                </a>
<ul class="articles">
<li class="chapter" data-level="1.3.1" data-path="特征工程_归一化.html">
<a href="特征工程_归一化.html">
<b>1.3.1.</b>
                    
                    特征工程_归一化
            
                </a>
</li>
<li class="chapter" data-level="1.3.2" data-path="特征工程_编码.html">
<a href="特征工程_编码.html">
<b>1.3.2.</b>
                    
                    特征工程_编码
            
                </a>
</li>
<li class="chapter" data-level="1.3.3" data-path="特征工程_特征组合.html">
<a href="特征工程_特征组合.html">
<b>1.3.3.</b>
                    
                    特征工程_特征组合
            
                </a>
</li>
<li class="chapter" data-level="1.3.4" data-path="特征工程_特征选择.html">
<a href="特征工程_特征选择.html">
<b>1.3.4.</b>
                    
                    特征工程_特征选择
            
                </a>
</li>
<li class="chapter" data-level="1.3.5" data-path="特征工程_文本表示模型.html">
<a href="特征工程_文本表示模型.html">
<b>1.3.5.</b>
                    
                    特征工程_文本表示模型
            
                </a>
</li>
<li class="chapter" data-level="1.3.6" data-path="特征工程_图像增强.html">
<a href="特征工程_图像增强.html">
<b>1.3.6.</b>
                    
                    特征工程_图像增强
            
                </a>
</li>
</ul>
</li>
<li class="chapter" data-level="1.4" data-path="模型评估.html">
<a href="模型评估.html">
<b>1.4.</b>
                    
                    模型评估
            
                </a>
<ul class="articles">
<li class="chapter" data-level="1.4.1" data-path="模型评估_评估指标.html">
<a href="模型评估_评估指标.html">
<b>1.4.1.</b>
                    
                    模型评估_评估指标
            
                </a>
</li>
<li class="chapter" data-level="1.4.2" data-path="模型评估_AB测试.html">
<a href="模型评估_AB测试.html">
<b>1.4.2.</b>
                    
                    模型评估_AB测试
            
                </a>
</li>
<li class="chapter" data-level="1.4.3" data-path="模型评估_过拟合和欠拟合.html">
<a href="模型评估_过拟合和欠拟合.html">
<b>1.4.3.</b>
                    
                    模型评估_过拟合和欠拟合
            
                </a>
</li>
<li class="chapter" data-level="1.4.4" data-path="模型评估_超参数选择.html">
<a href="模型评估_超参数选择.html">
<b>1.4.4.</b>
                    
                    模型评估_超参数选择
            
                </a>
</li>
<li class="chapter" data-level="1.4.5" data-path="模型评估_模型评估方法.html">
<a href="模型评估_模型评估方法.html">
<b>1.4.5.</b>
                    
                    模型评估_模型评估方法
            
                </a>
</li>
</ul>
</li>
<li class="chapter" data-level="1.5" data-path="降维.html">
<a href="降维.html">
<b>1.5.</b>
                    
                    降维
            
                </a>
<ul class="articles">
<li class="chapter" data-level="1.5.1" data-path="降维_PCA主成分分析.html">
<a href="降维_PCA主成分分析.html">
<b>1.5.1.</b>
                    
                    降维_PCA主成分分析
            
                </a>
</li>
<li class="chapter" data-level="1.5.2" data-path="降维_LDA线性判别分析.html">
<a href="降维_LDA线性判别分析.html">
<b>1.5.2.</b>
                    
                    降维_LDA线性判别分析
            
                </a>
</li>
</ul>
</li>
<li class="chapter" data-level="1.6" data-path="监督学习.html">
<a href="监督学习.html">
<b>1.6.</b>
                    
                    监督学习
            
                </a>
<ul class="articles">
<li class="chapter" data-level="1.6.1" data-path="监督学习_朴素贝叶斯分类.html">
<a href="监督学习_朴素贝叶斯分类.html">
<b>1.6.1.</b>
                    
                    监督学习_朴素贝叶斯分类
            
                </a>
</li>
<li class="chapter" data-level="1.6.2" data-path="监督学习_决策树.html">
<a href="监督学习_决策树.html">
<b>1.6.2.</b>
                    
                    监督学习_决策树
            
                </a>
</li>
<li class="chapter active" data-level="1.6.3" data-path="监督学习_K近邻法.html">
<a href="监督学习_K近邻法.html">
<b>1.6.3.</b>
                    
                    监督学习_K近邻法
            
                </a>
</li>
<li class="chapter" data-level="1.6.4" data-path="监督学习_支持向量机.html">
<a href="监督学习_支持向量机.html">
<b>1.6.4.</b>
                    
                    监督学习_支持向量机
            
                </a>
</li>
<li class="chapter" data-level="1.6.5" data-path="监督学习_CRF.html">
<a href="监督学习_CRF.html">
<b>1.6.5.</b>
                    
                    监督学习_CRF
            
                </a>
</li>
</ul>
</li>
<li class="chapter" data-level="1.7" data-path="非监督学习.html">
<a href="非监督学习.html">
<b>1.7.</b>
                    
                    非监督学习
            
                </a>
<ul class="articles">
<li class="chapter" data-level="1.7.1" data-path="非监督学习_K均值.html">
<a href="非监督学习_K均值.html">
<b>1.7.1.</b>
                    
                    非监督学习_K均值
            
                </a>
</li>
<li class="chapter" data-level="1.7.2" data-path="非监督学习_Mean Shift均值漂移聚类.html">
<a href="非监督学习_Mean Shift均值漂移聚类.html">
<b>1.7.2.</b>
                    
                    非监督学习_Mean Shift均值漂移聚类
            
                </a>
</li>
<li class="chapter" data-level="1.7.3" data-path="非监督学习_DBSCAN基于密度的聚类方法.html">
<a href="非监督学习_DBSCAN基于密度的聚类方法.html">
<b>1.7.3.</b>
                    
                    非监督学习_DBSCAN基于密度的聚类方法
            
                </a>
</li>
<li class="chapter" data-level="1.7.4" data-path="非监督学习_Hierarchical Clustering层次聚类.html">
<a href="非监督学习_Hierarchical Clustering层次聚类.html">
<b>1.7.4.</b>
                    
                    非监督学习_Hierarchical Clustering层次聚类
            
                </a>
</li>
<li class="chapter" data-level="1.7.5" data-path="非监督学习_Spectral Clustering谱聚类.html">
<a href="非监督学习_Spectral Clustering谱聚类.html">
<b>1.7.5.</b>
                    
                    非监督学习_Spectral Clustering谱聚类
            
                </a>
</li>
</ul>
</li>
<li class="chapter" data-level="1.8" data-path="半监督学习.html">
<a href="半监督学习.html">
<b>1.8.</b>
                    
                    半监督学习
            
                </a>
</li>
<li class="chapter" data-level="1.9" data-path="集成学习.html">
<a href="集成学习.html">
<b>1.9.</b>
                    
                    集成学习
            
                </a>
</li>
<li class="chapter" data-level="1.10" data-path="强化学习.html">
<a href="强化学习.html">
<b>1.10.</b>
                    
                    强化学习
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            本书使用 GitBook 发布
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">监督学习_K近邻法</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#k近邻法knn">1 K近邻法(KNN)</a></li><li><span class="title-icon"></span><a href="#k近邻模型">2 k近邻模型</a></li><ul><li><span class="title-icon"></span><a href="#模型">2.1 模型</a></li><li><span class="title-icon"></span><a href="#距离度量">2.2 距离度量</a></li><li><span class="title-icon"></span><a href="#k值的选择">2.3 k值的选择</a></li><li><span class="title-icon"></span><a href="#分类决策规则">2.4 分类决策规则</a></li></ul><li><span class="title-icon"></span><a href="#构建kd树">3 构建kd树</a></li><li><span class="title-icon"></span><a href="#搜索kd树">4 搜索kd树</a></li><li><span class="title-icon"></span><a href="#概要">5 概要</a></li></ul></div><a href="#k近邻法knn" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><p><a data-lightbox="5aee72d5-f8f9-4fbf-825c-dbc26e524367" data-title="异世界.png" href="res/other/异世界蕾姆_1.png"><img alt="异世界.png" src="res/other/异世界蕾姆_1.png"/></a></p>
<p><li><span class="title-icon"></span><a href="#k近邻法knn">1 K近邻法(KNN)</a></li><li><span class="title-icon"></span><a href="#k近邻模型">2 k近邻模型</a></li><ul><li><span class="title-icon"></span><a href="#模型">2.1 模型</a></li><li><span class="title-icon"></span><a href="#距离度量">2.2 距离度量</a></li><li><span class="title-icon"></span><a href="#k值的选择">2.3 k值的选择</a></li><li><span class="title-icon"></span><a href="#分类决策规则">2.4 分类决策规则</a></li></ul><li><span class="title-icon"></span><a href="#构建kd树">3 构建kd树</a></li><li><span class="title-icon"></span><a href="#搜索kd树">4 搜索kd树</a></li><li><span class="title-icon"></span><a href="#概要">5 概要</a></li></p>
<h1 id="k近邻法knn">1 K近邻法(KNN)</h1>
<blockquote>
<p><code>k近邻法</code>(k-nearest neighbor，k-NN)是一种基本分类与回归方法。</p>
<p>k近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。</p>
<p>k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻法<code>不具有显式的学习过程</code>。</p>
<p>k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。</p>
<p>k值的选择、距离度量及分类决策规则是k近邻法的<code>三个基本要素</code>。</p>
<p>k近邻法1968年由Cover和Hart提出。</p>
<p>输入：训练数据集
<script type="math/tex; mode=display">
>T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}
></script>
其中，<script type="math/tex; ">x_i \in x \subseteq R^n</script>为实例的特征向量，<script type="math/tex; ">y_i \in Y =\{ c_1,c_2,...,c_k \} </script>为实例的类别，<script type="math/tex; ">i＝1,2,...,N</script>；实例特征向量<script type="math/tex; ">x</script>；</p>
<p>输出：实例<script type="math/tex; ">x</script>所属的类<script type="math/tex; ">y</script>。</p>
<ol>
<li><p>根据给定距离度量，在训练集<script type="math/tex; ">T</script>中找出与<script type="math/tex; ">x</script>最邻近的<script type="math/tex; ">k</script>个点，涵盖这<script type="math/tex; ">k</script>个点的<script type="math/tex; ">x</script>的邻域记作<script type="math/tex; ">N_{k}(x)</script>；</p>
</li>
<li><p>在<script type="math/tex; ">N_{k}(x)</script>中根据分类决策规则(如多数表决)决定<script type="math/tex; ">x</script>的类别<script type="math/tex; ">y</script>：
<script type="math/tex; mode=display">
>   y=\arg \max_{c_j}\sum_{x_i \in N_k(x)}{I(y_i=c_j)},i=1,2,...,N;j=1,2,...,K
> </script></p>
</li>
<li><p>式（3.1）中，I为指示函数，即当y i ＝c j 时I为1，否则I为0。k近邻法的特殊情况是k＝1的情形，称为最近邻算法。对于输入的实例点（特征向量）x，最近邻法将训练数据集中与x最邻近点的类作为x的类。</p>
</li>
</ol>
</blockquote>
<h1 id="k近邻模型">2 k近邻模型</h1>
<blockquote>
<p>k近邻法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。</p>
</blockquote>
<h2 id="模型">2.1 模型</h2>
<blockquote>
<p>k近邻法中，当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。</p>
<p>这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。这一事实从最近邻算法中可以看得很清楚。</p>
<p>特征空间中，对每个训练实例点<script type="math/tex; ">ix</script>，距离该点比其他点更近的所有点组成一个区域，叫作单元(cell)。</p>
<p>每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。</p>
<p>最近邻法将实例<script type="math/tex; ">ix</script>的类<script type="math/tex; ">iy</script>作为其单元中所有点的类标记(class label)。</p>
<p>这样，每个单元的实例点的类别是确定的。下图是二维特征空间划分的一个例子。</p>
<p><a data-lightbox="f12cfb38-ffb8-4aad-b4d4-38eb20101c5f" data-title="1570101631106" href="res/Machine%20Learning%20Base/1570101631106.png"><img alt="1570101631106" src="res/Machine%20Learning%20Base/1570101631106.png"/></a></p>
</blockquote>
<h2 id="距离度量">2.2 距离度量</h2>
<blockquote>
<p>特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是<script type="math/tex; ">n</script>维实数向量空间<script type="math/tex; ">R^n</script> 。</p>
<p>使用的距离是欧氏距离，但也可以是其他距离，如更一般的<script type="math/tex; ">L_p</script>距离(<script type="math/tex; ">L_p</script> pdistance)或Minkowski距离(Minkowski distance)。</p>
<p>设特征空间<script type="math/tex; ">x</script>是<script type="math/tex; ">n</script>维实数向量空间<script type="math/tex; ">R^n</script> ，<script type="math/tex; ">x_i,x_j \in X</script>，</p>
<p><script type="math/tex; ">x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T</script>，</p>
<p><script type="math/tex; ">x_j=(x_j^{(1)},x_j^{(2)},...,x_j^{(n)})^T</script>，<script type="math/tex; ">x_i</script>，<script type="math/tex; ">x_j</script>的<script type="math/tex; ">L_p</script>距离定义为
<script type="math/tex; mode=display">
>L_p(x_i,x_j)=(\sum_{l=1}^{n}{|x_i^{(l)}-x_j^{(l)}|^{p}})^{\frac{1}{p}}
></script>
这里<script type="math/tex; ">p \geq 1</script>。</p>
<p>当<script type="math/tex; ">p＝2</script>时，称为欧氏距离(Euclidean distance)，即
<script type="math/tex; mode=display">
>L_2(x_i,x_j)=(\sum_{l=1}^{n}{|x_i^{(l)}-x_j^{(l)}|^{2}})^{\frac{1}{2}}
></script>
当<script type="math/tex; ">p＝1</script>时，称为曼哈顿距离(Manhattan distance)，即
<script type="math/tex; mode=display">
>L_p(x_i,x_j)=\sum_{l=1}^{n}{|x_i^{(l)}-x_j^{(l)}|}
></script>
当<script type="math/tex; ">p＝\infty</script>时，它是各个坐标距离的最大值，即
<script type="math/tex; mode=display">
>L_{\infty}(x_i,x_j)=\max_{l}{|x_i^{(l)}-x_j^{(l)}|}
></script>
下图给出了二维空间中<script type="math/tex; ">p</script>取不同值时，与原点的<script type="math/tex; ">L_p</script>距离为1(<script type="math/tex; ">L_p＝1</script>)的点的图形。</p>
<p><a data-lightbox="d6f33e61-12ab-457f-9caa-bf0855fceb61" data-title="1570102425834" href="res/Machine%20Learning%20Base/1570102425834.png"><img alt="1570102425834" src="res/Machine%20Learning%20Base/1570102425834.png"/></a></p>
</blockquote>
<h2 id="k值的选择">2.3 k值的选择</h2>
<blockquote>
<p>k值的选择会对k近邻法的结果产生重大影响。</p>
<p>如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差(approximation error)会减小，只有与输入实例较近的(相似的)训练实例才会对预测结果起作用。</p>
<p>但缺点是“学习”的估计误差(estimation error)会增大，预测结果会对近邻的实例点非常敏感。</p>
<p>如果邻近的实例点恰巧是噪声，预测就会出错。</p>
<p>换句话说，k值的减小就意味着整体模型变得复杂，容易发生过拟合。</p>
<p>如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误。k值的增大就意味着整体的模型变得简单。</p>
<p>如果<script type="math/tex; ">k＝N</script>，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。</p>
<p>这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。</p>
<p>在应用中，k值一般取一个比较小的数值。通常采用<code>交叉验证法</code>来选取最优的k值。</p>
</blockquote>
<h2 id="分类决策规则">2.4 分类决策规则</h2>
<blockquote>
<p>k近邻法中的分类决策规则往往是<code>多数表决</code>，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。</p>
<p>多数表决规则(majority voting rule)有如下解释：如果分类的损失函数为0-1损失函数，分类函数为
<script type="math/tex; mode=display">
>f:R^n \to \{ c_1,c_2,...,c_k \}
></script>
那么误分类的概率是
<script type="math/tex; mode=display">
>P(Y \neq f(X))=1-P(Y=f(X))
></script>
对给定的实例<script type="math/tex; ">x \in x</script>，其最近邻的k个训练实例点构成集合<script type="math/tex; ">N_k^{(x)}</script>。如果涵盖<script type="math/tex; ">N_k{(x)}</script>的区域的类别是<script type="math/tex; ">c_j</script> ，那么误分类率是
<script type="math/tex; mode=display">
>\frac{1}{k}\sum_{x_i \in N_k{(x)}}I(y_i \neq c_j)=1-\frac{1}{k}\sum_{x_i \in N_k{(x)}}{}I(y_i=c_j)
></script>
要使误分类率最小即经验风险最小，就要使<script type="math/tex; ">\sum_{x_i \in N_k{(x)}}{}I(y_i=c_j)</script>最大，所以多数表决规则等价于<code>经验风险最小化</code>。</p>
</blockquote>
<h1 id="构建kd树">3 构建kd树</h1>
<blockquote>
<p>实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。这点在特征空间的维数大及训练数据容量大时尤其必要。</p>
<p>k近邻法最简单的实现方法是线性扫描(linear scan)。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。</p>
<p>为了提高k近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。具体方法很多，下面介绍其中的kd树(kd tree)方法。</p>
<p><code>kd树</code>是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。</p>
<p>kd树是二叉树，表示对k维空间的一个划分(partition)。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。</p>
<p><code>构造kd树</code>的方法如下：</p>
<ol>
<li>构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域；</li>
<li>通过下面的递归方法，不断地对k维空间进行切分，生成子结点。</li>
<li>在超矩形区域(结点)上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域(子结点)；这时，实例被分到两个子区域。</li>
<li>这个过程直到子区域内没有实例时终止(终止时的结点为叶结点)。</li>
<li>在此过程中，将实例保存在相应的结点上。</li>
</ol>
<p>通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数(median)为切分点，这样得到的kd树是平衡的。注意平衡的kd树搜索时的效率未必是最优的。</p>
<p><code>Example1</code>: 给定一个二维空间的数据集：
<script type="math/tex; mode=display">
>T=\{ (2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T \}
></script>
构造一个平衡kd树</p>
<p><strong>解</strong> 　根结点对应包含数据集<script type="math/tex; ">T</script>的矩形，选择<script type="math/tex; ">x^{(1)}</script>轴，6个数据点的<script type="math/tex; ">x^{(1)}</script>坐标的中位数是7，以平面<script type="math/tex; ">x^{(1)}=7</script>将空间分为左、右两个子矩形(子结点)；接着，左矩形以<script type="math/tex; ">x^{(2)}=4</script>分为两个子矩形，右矩形以<script type="math/tex; ">x^{(2)}=6</script>分为两个子矩形，如此递归，最后得到下图所示的特征空间划分和下图所示的kd树。</p>
<p><a data-lightbox="035504bf-3197-431b-8cb0-8dcdb5b4dc45" data-title="1570104114031" href="res/Machine%20Learning%20Base/1570104114031.png"><img alt="1570104114031" src="res/Machine%20Learning%20Base/1570104114031.png"/></a></p>
<p><a data-lightbox="d6b4b721-3b53-42a2-96ae-65a03ce6459e" data-title="1570104123858" href="res/Machine%20Learning%20Base/1570104123858.png"><img alt="1570104123858" src="res/Machine%20Learning%20Base/1570104123858.png"/></a></p>
</blockquote>
<h1 id="搜索kd树">4 搜索kd树</h1>
<blockquote>
<p>利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。这里以最近邻为例加以叙述，同样的方法可以应用到k近邻。</p>
<p>给定一个目标点，搜索其最近邻。首先找到包含目标点的叶结点；然后从该叶结点出发，依次回退到父结点；不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。</p>
<p>包含目标点的叶结点对应包含目标点的最小超矩形区域。</p>
<p>以此叶结点的实例点作为当前最近点。</p>
<p>目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部。</p>
<p>然后返回当前结点的父结点，如果父结点的另一子结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例点。</p>
<p>如果存在这样的点，将此点作为新的当前最近点。</p>
<p>算法转到更上一级的父结点，继续上述过程。</p>
<p>如果父结点的另一子结点的超矩形区域与超球体不相交，或不存在比当前最近点更近的点，则停止搜索。</p>
<p><code>Example1</code>: 给定一个如图所示的kd树，根结点为A，其子结点为B，C等。</p>
<p>树上共存储7个实例点；另有一个输入目标实例点S，求S的最近邻。</p>
<p><a data-lightbox="f2ad23c2-cd6f-4c67-bc48-653395bf5815" data-title="1570104452917" href="res/Machine%20Learning%20Base/1570104452917.png"><img alt="1570104452917" src="res/Machine%20Learning%20Base/1570104452917.png"/></a></p>
<p><strong>解</strong> 　首先在kd树中找到包含点S的叶结点D(图中的右下区域)，以点D作为近似最近邻。</p>
<p>真正最近邻一定在以点S为中心通过点D的圆的内部。</p>
<p>然后返回结点D的父结点B，在结点B的另一子结点F的区域内搜索最近邻。</p>
<p>结点F的区域与圆不相交，不可能有最近邻点。</p>
<p>继续返回上一级父结点A，在结点A的另一子结点C的区域内搜索最近邻。</p>
<p>结点C的区域与圆相交；该区域在圆内的实例点有点E，点E比点D更近，成为新的最近邻近似。</p>
<p>最后得到点E是点S的最近邻。</p>
</blockquote>
<h1 id="概要">5 概要</h1>
<blockquote>
<p>1．k近邻法是基本且简单的分类与回归方法。k近邻法的基本做法是：对给定的训练实例点和输入实例点，首先确定输入实例点的k个最近邻训练实例点，然后利用这k个训练实例点的类的多数来预测输入实例点的类。</p>
<p>2．k近邻模型对应于基于训练数据集对特征空间的一个划分。k近邻法中，当训练集、距离度量、k值及分类决策规则确定后，其结果唯一确定。</p>
<p>3．k近邻法三要素：距离度量、k值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的pL距离。k值小时，k近邻模型更复杂；k值大时，k近邻模型更简单。k值的选择反映了对近似误差与估计误差之间的权衡，通常由交叉验证选择最优的k。常用的分类决策规则是多数表决，对应于经验风险最小化。</p>
<p>4．k近邻法的实现需要考虑如何快速搜索k个最近邻点。kd树是一种便于对k维空间中的数据进行快速检索的数据结构。kd树是二叉树，表示对k维空间的一个划分，其每个结点对应于k维空间划分中的一个超矩形区域。利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p>
</blockquote>
<p></p><footer class="page-footer"><span class="copyright">Copyright © narutohyc.com 2020 all right reserved，powered by Gitbook</span><span class="footer-modification">该文件修订时间：
2022-02-16 06:00:27
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz',appKey: 'FOXMptWOHC7cU1FxXt0LJj4o',placeholder: 'Just go go',avatar: '',meta: undefined,pageSize: 10,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: 监督学习_决策树" class="navigation navigation-prev" href="监督学习_决策树.html">
<i class="fa fa-angle-left"></i>
</a>
<a aria-label="Next page: 监督学习_支持向量机" class="navigation navigation-next" href="监督学习_支持向量机.html">
<i class="fa fa-angle-right"></i>
</a>
</div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"监督学习_K近邻法","level":"1.6.3","depth":2,"next":{"title":"监督学习_支持向量机","level":"1.6.4","depth":2,"path":"chapters/监督学习_支持向量机.md","ref":"chapters/监督学习_支持向量机.md","articles":[]},"previous":{"title":"监督学习_决策树","level":"1.6.2","depth":2,"path":"chapters/监督学习_决策树.md","ref":"chapters/监督学习_决策树.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","favicon","search-plus","-lunr","-search","lightbox","change_girls","theme-comscore","valine","pageview-count","favicon-absolute"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright © narutohyc.com 2020","modify_label":"该文件修订时间：","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/narutohyc"},"splitter":{},"change_girls":{"time":10,"urls":["https://plc.jj20.com/up/allimg/1115/012122143136/220121143136-2.jpg","https://plc.jj20.com/up/allimg/1115/111R1094405/21111P94405-1.jpg"]},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://i.loli.net/2021/01/12/Cdunm9AoBcHF5MI.png","alipayText":"alipay打赏","button":"欢迎打赏","title":"","wechat":"https://i.loli.net/2021/01/12/gmzASfCciIFXTyr.png","wechatText":"wechat打赏"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"favicon":{"shortcut":"https://upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png","bookmark":"https://upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png","appleTouch":"https://upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png","appleTouchMore":{"120x120":"https://upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png","180x180":"https://upload-images.jianshu.io/upload_images/15675864-5e5ba668035853d8.png"}},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"narutohyc","repo":"bk_machineLearning","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"","lang":"zh-CN","pageSize":10,"placeholder":"Just go go","recordIP":false,"appId":"jQTfSSGRF0zp8yqaaa6bjeVQ-gzGzoHsz","appKey":"FOXMptWOHC7cU1FxXt0LJj4o"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"机器学习相关学习记录","language":"zh-hans","links":{"sidebar":{"我的狗窝":"https://github.com/narutohyc"}},"gitbook":"*","description":"记录 机器学习 的学习和一些技巧的使用"},"file":{"path":"chapters/监督学习_K近邻法.md","mtime":"2022-02-16T06:00:27.763Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2022-02-16T06:02:02.340Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-change_girls/girls.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
